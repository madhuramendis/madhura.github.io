{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Siddhi - Cloud native stream processing Geting Started | Download | User Guide | Contribute Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi can run as an embedded Java library , and as a microservice on bare metal, VM , Docker and natively in Kubernetes . It also has a graphical and text editor for building Streaming Data Integration and Streaming Analytics applications. Distributions And more installation options Overview Siddhi supports: Streaming Data Integration Retrieving data from various event sources (NATS, Kafka, JMS, HTTP, CDC, etc) Map events to and from multiple event formats (JSON, XML, Text, Avro, etc) Data preprocessing cleaning Joining multiple data streams Integrate streaming data with databases (RDBMS, Cassandra, HBase, Redis, etc) Integrate with external services Publish data to multiple event sinks (Email, JMS, HTTP, etc) Streaming Data Analytics Generating alerts based on thresholds Calculate aggregations over a short windows (time, length, session, unique, etc) or a long time period Calculate aggregations over long time periods with seconds, minutes, hours, days, months years granularity Correlating data while finding missing and erroneous events Detecting temporal event patterns Analyzing trends (rise, fall, turn, tipple bottom) Run pretreated machine learning models (PMML, Tensorflow) Learn and predict at runtime using online machine learning models Adaptive Intelligence Static rule processing Stateful rule processing Decision making through synchronous stream processing Query tables, windows and aggregations And many more ... For more information, see Patterns of Streaming Realtime Analytics Siddhi is free and open source, released under Apache Software License v2.0 . Why use Siddhi ? Fast . UBER uses it to process 20 Billion events per day (300,000 events per second). Lightweight (core Siddhi libs are 2MB), and embeddable in Android, Python and RaspberryPi. Has over 50 Siddhi Extensions Used by over 60 companies including many Fortune 500 companies in production. Following are some examples: WSO2 uses Siddhi for the following purposes: To provide distributed and high available stream processing capabilities via WSO2 Stream Processor . It is named as a strong performer in The Forrester Wave: Big Data Streaming Analytics, Q1 2016 ( Report ). As the edge analytics library of WSO2 IoT Server . As the core of WSO2 API Manager 's throttling. As the core of WSO2 products' analytics. UBER uses Siddhi for fraud analytics. Apache Eagle uses Siddhi as a policy engine. Also used by Punch Platform , Sqooba , and SiteWhere Solutions based on Siddhi have been finalists at ACM DEBS Grand Challenge Stream Processing competitions in 2014, 2015, 2016, 2017 . Siddhi has been the basis of many academic research projects and has over 60 citations . If you are a Siddhi user, we would love to hear more on how you use Siddhi? Please share your experience and feedback via the Siddhi user Google group . Get Started! Get started with Siddhi in a few minutes by following the Siddhi Quick Start Guide Siddhi Development Environment Siddhi Tooling Siddhi provides siddhi-tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. IntelliJ IDEA Plugin Install IDEA plugin to get the following features: Siddhi Query Editor with syntax highlighting and with basic auto completion Siddhi Runner and Debugger support to test Siddhi Application Siddhi Versions Active development version of Siddhi : v5.0.0 built on Java 8 11. Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v5.x.x Architecture of Siddhi v5.x.x Latest Stable Release of Siddhi v4.x.x : v4.4.8 built on Java 8. (Recommended for production use) Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v4.x.x Architecture of Siddhi v4.x.x Latest Stable Release of Siddhi v3.x.x : v3.2.3 built on Java 7. Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v3.x.x Latest API Docs Latest API Docs is 5.0.0 . Contact us Post your questions with the \"Siddhi\" tag in Stackoverflow . For questions and feedback please connect via the Siddhi user Google group . Engage in community development through Siddhi dev Google group . How to Contribute Find the detail information on asking questions, providing feedback, reporting issues, building and contributing code on How to contribute? section. Roadmap Support Kafka Support NATS Siddhi Runner Distribution Siddhi Tooling (Editor) Siddhi Kubernetes CRD Periodic incremental state persistence Support Prometheus for metrics collection Support high available Siddhi deployment with NATS via Kubernetes CRD Support distributed Siddhi deployment with NATS via Kubernetes CRD Support WSO2 provides production, and query support for Siddhi and its extensions . For more details contact via http://wso2.com/support/ Siddhi is joint research project initiated by WSO2 and University of Moratuwa , Sri Lanka.","title":"Material"},{"location":"#siddhi-cloud-native-stream-processing","text":"Geting Started | Download | User Guide | Contribute Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi can run as an embedded Java library , and as a microservice on bare metal, VM , Docker and natively in Kubernetes . It also has a graphical and text editor for building Streaming Data Integration and Streaming Analytics applications.","title":"Siddhi - Cloud native stream processing"},{"location":"#distributions","text":"And more installation options","title":"Distributions"},{"location":"#overview","text":"Siddhi supports: Streaming Data Integration Retrieving data from various event sources (NATS, Kafka, JMS, HTTP, CDC, etc) Map events to and from multiple event formats (JSON, XML, Text, Avro, etc) Data preprocessing cleaning Joining multiple data streams Integrate streaming data with databases (RDBMS, Cassandra, HBase, Redis, etc) Integrate with external services Publish data to multiple event sinks (Email, JMS, HTTP, etc) Streaming Data Analytics Generating alerts based on thresholds Calculate aggregations over a short windows (time, length, session, unique, etc) or a long time period Calculate aggregations over long time periods with seconds, minutes, hours, days, months years granularity Correlating data while finding missing and erroneous events Detecting temporal event patterns Analyzing trends (rise, fall, turn, tipple bottom) Run pretreated machine learning models (PMML, Tensorflow) Learn and predict at runtime using online machine learning models Adaptive Intelligence Static rule processing Stateful rule processing Decision making through synchronous stream processing Query tables, windows and aggregations And many more ... For more information, see Patterns of Streaming Realtime Analytics Siddhi is free and open source, released under Apache Software License v2.0 .","title":"Overview"},{"location":"#why-use-siddhi","text":"Fast . UBER uses it to process 20 Billion events per day (300,000 events per second). Lightweight (core Siddhi libs are 2MB), and embeddable in Android, Python and RaspberryPi. Has over 50 Siddhi Extensions Used by over 60 companies including many Fortune 500 companies in production. Following are some examples: WSO2 uses Siddhi for the following purposes: To provide distributed and high available stream processing capabilities via WSO2 Stream Processor . It is named as a strong performer in The Forrester Wave: Big Data Streaming Analytics, Q1 2016 ( Report ). As the edge analytics library of WSO2 IoT Server . As the core of WSO2 API Manager 's throttling. As the core of WSO2 products' analytics. UBER uses Siddhi for fraud analytics. Apache Eagle uses Siddhi as a policy engine. Also used by Punch Platform , Sqooba , and SiteWhere Solutions based on Siddhi have been finalists at ACM DEBS Grand Challenge Stream Processing competitions in 2014, 2015, 2016, 2017 . Siddhi has been the basis of many academic research projects and has over 60 citations . If you are a Siddhi user, we would love to hear more on how you use Siddhi? Please share your experience and feedback via the Siddhi user Google group .","title":"Why use Siddhi ?"},{"location":"#get-started","text":"Get started with Siddhi in a few minutes by following the Siddhi Quick Start Guide","title":"Get Started!"},{"location":"#siddhi-development-environment","text":"Siddhi Tooling Siddhi provides siddhi-tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. IntelliJ IDEA Plugin Install IDEA plugin to get the following features: Siddhi Query Editor with syntax highlighting and with basic auto completion Siddhi Runner and Debugger support to test Siddhi Application","title":"Siddhi Development Environment"},{"location":"#siddhi-versions","text":"Active development version of Siddhi : v5.0.0 built on Java 8 11. Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v5.x.x Architecture of Siddhi v5.x.x Latest Stable Release of Siddhi v4.x.x : v4.4.8 built on Java 8. (Recommended for production use) Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v4.x.x Architecture of Siddhi v4.x.x Latest Stable Release of Siddhi v3.x.x : v3.2.3 built on Java 7. Find the released Siddhi libraries here . Siddhi Query Guide for Siddhi v3.x.x","title":"Siddhi Versions"},{"location":"#latest-api-docs","text":"Latest API Docs is 5.0.0 .","title":"Latest API Docs"},{"location":"#contact-us","text":"Post your questions with the \"Siddhi\" tag in Stackoverflow . For questions and feedback please connect via the Siddhi user Google group . Engage in community development through Siddhi dev Google group .","title":"Contact us"},{"location":"#how-to-contribute","text":"Find the detail information on asking questions, providing feedback, reporting issues, building and contributing code on How to contribute? section.","title":"How to Contribute"},{"location":"#roadmap","text":"Support Kafka Support NATS Siddhi Runner Distribution Siddhi Tooling (Editor) Siddhi Kubernetes CRD Periodic incremental state persistence Support Prometheus for metrics collection Support high available Siddhi deployment with NATS via Kubernetes CRD Support distributed Siddhi deployment with NATS via Kubernetes CRD","title":"Roadmap"},{"location":"#support","text":"WSO2 provides production, and query support for Siddhi and its extensions . For more details contact via http://wso2.com/support/ Siddhi is joint research project initiated by WSO2 and University of Moratuwa , Sri Lanka.","title":"Support"},{"location":"home/","text":"Siddhi Cloud native stream processing Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Download Lorem ipsum dolor sit amet, consectetur adipiscing Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipiscing Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"home"},{"location":"documentation/api-5.0.0/","text":"API Docs - v5.0.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type= passThrough ) Examples EXAMPLE 1 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"API Guide"},{"location":"documentation/api-5.0.0/#api-docs-v500","text":"","title":"API Docs - v5.0.0"},{"location":"documentation/api-5.0.0/#core","text":"","title":"Core"},{"location":"documentation/api-5.0.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"documentation/api-5.0.0/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"documentation/api-5.0.0/#count-aggregate-function","text":"Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"documentation/api-5.0.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"documentation/api-5.0.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"documentation/api-5.0.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"documentation/api-5.0.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"documentation/api-5.0.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"documentation/api-5.0.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"documentation/api-5.0.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"documentation/api-5.0.0/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"documentation/api-5.0.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"documentation/api-5.0.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"documentation/api-5.0.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"documentation/api-5.0.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"documentation/api-5.0.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"documentation/api-5.0.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"documentation/api-5.0.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"documentation/api-5.0.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"documentation/api-5.0.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"documentation/api-5.0.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"documentation/api-5.0.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"documentation/api-5.0.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"documentation/api-5.0.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"documentation/api-5.0.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"documentation/api-5.0.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"documentation/api-5.0.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"documentation/api-5.0.0/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"documentation/api-5.0.0/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"documentation/api-5.0.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"documentation/api-5.0.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"documentation/api-5.0.0/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"documentation/api-5.0.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"documentation/api-5.0.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"documentation/api-5.0.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"documentation/api-5.0.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"documentation/api-5.0.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"documentation/api-5.0.0/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"documentation/api-5.0.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"documentation/api-5.0.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"documentation/api-5.0.0/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"documentation/api-5.0.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"documentation/api-5.0.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"documentation/api-5.0.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"documentation/api-5.0.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"documentation/api-5.0.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"documentation/api-5.0.0/#sink","text":"","title":"Sink"},{"location":"documentation/api-5.0.0/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"documentation/api-5.0.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"documentation/api-5.0.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"documentation/api-5.0.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"documentation/api-5.0.0/#source","text":"","title":"Source"},{"location":"documentation/api-5.0.0/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"documentation/api-5.0.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"documentation/api-5.0.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type= passThrough ) Examples EXAMPLE 1 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"documentation/architecture-5.x/","text":"Siddhi 5.x Architecture Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run with WSO2 Stream Processor as a service Embedded into any Java or Python-based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. Info Please find the Siddhi 4.x Architecture here This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter Windows) A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern Sequence) The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurging is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Siddhi 5.x Architecture"},{"location":"documentation/architecture-5.x/#siddhi-5x-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run with WSO2 Stream Processor as a service Embedded into any Java or Python-based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. Info Please find the Siddhi 4.x Architecture here This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.x Architecture"},{"location":"documentation/architecture-5.x/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"documentation/architecture-5.x/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"documentation/architecture-5.x/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"documentation/architecture-5.x/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"documentation/architecture-5.x/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"documentation/architecture-5.x/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"documentation/architecture-5.x/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"documentation/architecture-5.x/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price = 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"documentation/architecture-5.x/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"documentation/architecture-5.x/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"documentation/architecture-5.x/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"documentation/architecture-5.x/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"documentation/architecture-5.x/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurging is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"documentation/architecture-5.x/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"documentation/architecture-5.x/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"documentation/config-guide-5.x/","text":"Siddhi 5.x Config Guide This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles Configuring Databases Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL Configuring Periodic State Persistence Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. Persistence on File System To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. Configuring Siddhi Elements Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: name type: type properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref= name , @map(type= json , @attributes( name= $.name , amount= $.quantity ))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: http-passthrough type: http properties: receiver.url: http://0.0.0.0:8008/sweet-production basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref= http-passthrough , @map(type= json , @attributes( name= $.name , amount= $.quantity ))) define stream SweetProductionStream (name string, amount double); Configuring Extensions Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) mysql.recordDeleteQuery: DELETE FROM {{TABLE_NAME}} {{CONDITION}} mysql.recordExistsQuery: SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1 Configuring Authentication Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: local # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin Adding Extensions and Third Party Dependencies Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: OFF jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Reporting via Console To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5 Reporting via Database To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Configuration Guide"},{"location":"documentation/config-guide-5.x/#siddhi-5x-config-guide","text":"This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles","title":"Siddhi 5.x Config Guide"},{"location":"documentation/config-guide-5.x/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in SIDDHI_RUNNER_HOME /wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to SIDDHI_RUNNER_HOME /lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL","title":"Configuring Databases"},{"location":"documentation/config-guide-5.x/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"documentation/config-guide-5.x/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence.","title":"Persistence on Database"},{"location":"documentation/config-guide-5.x/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence.","title":"Persistence on File System"},{"location":"documentation/config-guide-5.x/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"documentation/config-guide-5.x/#configuring-sources-sinks-and-stores","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: name type: type properties: property1 : value1 property2 : value2 For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref= name , @map(type= json , @attributes( name= $.name , amount= $.quantity ))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: http-passthrough type: http properties: receiver.url: http://0.0.0.0:8008/sweet-production basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref= http-passthrough , @map(type= json , @attributes( name= $.name , amount= $.quantity ))) define stream SweetProductionStream (name string, amount double);","title":"Configuring Sources, Sinks and Stores"},{"location":"documentation/config-guide-5.x/#configuring-extensions","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: extension name namespace: extension namespace properties: key : value For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}}) mysql.recordDeleteQuery: DELETE FROM {{TABLE_NAME}} {{CONDITION}} mysql.recordExistsQuery: SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1","title":"Configuring Extensions"},{"location":"documentation/config-guide-5.x/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: local # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Configuring Authentication"},{"location":"documentation/config-guide-5.x/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"documentation/config-guide-5.x/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. !--HTTP extension-- dependency groupId org.wso2.extension.siddhi.io.http /groupId artifactId siddhi-io-http /artifactId version ${siddhi.io.http.version} /version /dependency Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"documentation/config-guide-5.x/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in SIDDHI_RUNNER_HOME /lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the SIDDHI_RUNNER_HOME /lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"documentation/config-guide-5.x/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"documentation/config-guide-5.x/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"documentation/config-guide-5.x/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"documentation/config-guide-5.x/#reporting-via-jmx-mbeans","text":"JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: OFF jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO","title":"Reporting via JMX Mbeans"},{"location":"documentation/config-guide-5.x/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5","title":"Reporting via Console"},{"location":"documentation/config-guide-5.x/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the SIDDHI_RUNNER_HOME /wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database -- # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/ datasource JNDI name . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"documentation/config-guide-5.x/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the SIDDHI_RUNNER_HOME /bin directory, and issue the following command. ./jartobundle.sh path to non OSGi jar ../lib This converts the Jar to OSGi bundles and place it in SIDDHI_RUNNER_HOME /lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"documentation/download/","text":"Download Siddhi Select the appropriate Siddhi distribution for your use case. Siddhi Distribution Siddhi 5.x ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice Siddhi Docker Siddhi 5.x (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice Siddhi Kubernetes Siddhi 5.x (based on Distribution 0.1.0) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice Siddhi Libs Siddhi 5.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Siddhi 4.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library","title":"Download Siddhi"},{"location":"documentation/download/#download-siddhi","text":"Select the appropriate Siddhi distribution for your use case.","title":"Download Siddhi"},{"location":"documentation/download/#siddhi-distribution","text":"Siddhi 5.x ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice","title":"Siddhi Distribution"},{"location":"documentation/download/#siddhi-docker","text":"Siddhi 5.x (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice","title":"Siddhi Docker"},{"location":"documentation/download/#siddhi-kubernetes","text":"Siddhi 5.x (based on Distribution 0.1.0) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice","title":"Siddhi Kubernetes"},{"location":"documentation/download/#siddhi-libs","text":"Siddhi 5.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Siddhi 4.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library","title":"Siddhi Libs"},{"location":"documentation/extensions/","text":"Siddhi Extensions Available Extensions Following are some pre-written extensions that are supported with Siddhi; Extensions released under Apache 2.0 License Name Description execution-approximate The siddhi-execution-approximate is an extension to Siddhi that performs approximate computing on event streams. execution-env The siddhi-execution-env extension is an extension to Siddhi that provides the capability to read environment properties inside Siddhi stream definitions and use it inside queries. Functions of the env extension are as follows.. execution-extrema The siddhi-execution-extrema extension is an extension to Siddhi that processes event streams based on different arithmetic properties. Different types of processors are available to extract the extremas from the event streams according to the specified attribute in the stream. execution-geo The siddhi-execution-geo extension is an extension to Siddhi that provides geo data related functionality such as checking whether a given geo coordinate is within a predefined geo-fence, etc. Following are the functions of the Geo extension. execution-graph The siddhi-execution-graph extension is an extension to Siddhi that provides graph related functionality to Siddhi such as getting size of largest connected component of a graph, maximum clique size of a graph, etc. execution-kalmanfilter The siddhi-execution-kalman-filter extension is an extension to Siddhi provides that Kalman filtering capabilities to Siddhi. This allows you to detect outliers of input data. execution-map The siddhi-execution-map extension is an extension to Siddhi that provides the capability to send a map object inside Siddhi stream definitions and use it inside queries. The following are the functions of the map extension.. execution-markov The siddhi-execution-markov extension is an extension to Siddhi that allows abnormal patterns relating to user activity to be detected when carrying out real time analysis. execution-math The siddhi-execution-math is an extension to Siddhi, which provides useful mathematical functions that can make your siddhi queries more flexible. execution-priority The siddhi-execution-priority extension is an extension to Siddhi that keeps track of the priority of events in a stream. execution-regex The siddhi-execution-regex extension is an extension to Siddhi that provides basic RegEx execution capabilities. execution-reorder The siddhi-execution-reorder extension is an extension to Siddhi that is used for reordering events from an unordered event stream. Reorder extension is implemented using the K-Slack and alpha K-Stack algorithms. execution-sentiment The siddhi-execution-sentiment extension is an extension to Siddhi that performs sentiment analysis using Afinn Wordlist-based approach. execution-stats The siddhi-execution-stats extension is an extension to Siddhi that provides statistical functions for aggregated events. Currently this contains median function which calculate the median of aggregated events. Calculation of median is done for each event arrival and expiry, it is not recommended to use this extension for large window sizes. execution-streamingml The siddhi-execution-streamingml is an extension to Siddhi that performs streaming machine learning on event streams. execution-string The siddhi-execution-string is an extension to Siddhi that provides basic string handling capabilities such as con-cat, length, convert to lowercase, and replace all. execution-tensorflow The siddhi-execution-tensorflow is an extension to Siddhi that adds support for inferences from pre-built TensorFlow SavedModels using Siddhi. execution-time The siddhi-execution-time extension is an extension to Siddhi that provides time related functionality to Siddhi such as getting current time, current date, manipulating/formatting dates and etc. execution-timeseries The siddhi-execution-timeseries extension is an extension to Siddhi which enables users to forecast and detect outliers in time series data, using Linear Regression Models. execution-unique The siddhi-execution-unique extension is an extension to Siddhi that processes event streams based on unique events. Different types of unique windows are available to hold unique events based on the given unique key parameter. execution-unitconversion The siddhi-execution-unitconversion extension is an extension to Siddhi that enables conversions of length, mass, time and volume units. io-cdc The siddhi-io-cdc extension is an extension to Siddhi. It receives change data from MySQL, MS SQL Server, Postgresql, H2 and Oracle in the key-value format. io-email The siddhi-io-email extension is an extension to Siddhi that receives and publishes events via email. Using the extension, events can be published through smtp mail server and received through 'pop3' or 'imap' mail serves. io-file The siddhi-io-file extension is an extension to Siddhi which is used to receive/publish event data from/to file. It supports both binary and text formats. io-http The siddhi-io-http extension is an extension to Siddhi that allows you to receive and publish events via http and https transports and also allow you perform synchronous request and response. This extension works with WSO2 Stream Processor and with standalone Siddhi. io-jms The siddhi-io-jms extension is an extension to Siddhi that used to receive and publishe events via JMS Message. This extension allows users to subscribe to a JMS broker and receive/publish JMS messages. io-kafka The siddhi-io-kafka extension is an extension to Siddhi. This implements siddhi kafka source and sink that can be used to receive events from a kafka cluster and to publish events to a kafka cluster. io-mqtt The siddhi-io-mqtt is an extension to Siddhi mqtt source and sink implementation,that publish and receive events from mqtt broker. io-prometheus The siddhi-io-prometheus extension is an extension to Siddhi. The Prometheus-sink publishes Siddhi events as Prometheus metrics and expose them to Prometheus server. The Prometheus-source retrieves Prometheus metrics from an endpoint and send them as Siddhi events. io-rabbitmq The siddhi-io-rabbitmq is an extension to Siddhi that publish and receive events from rabbitmq broker. io-sqs The siddhi-io-sqs extension is an extension to Siddhi that used to receive and publish events via AWS SQS Service. This extension allows users to subscribe to a SQS queue and receive/publish SQS messages. io-tcp The siddhi-io-tcp extension is an extension to Siddhi that allows to receive and publish events through TCP. io-twitter The siddhi-io-twitter extension is an extension to Siddhi. It publishes event data from Twitter Applications in the key-value map format. io-websocket The siddhi-io-websocket extension is an extension to Siddhi that allows to receive and publish events through WebSocket. io-wso2event The siddhi-io-wso2event extension is an extension to Siddhi that receives and publishes events in the WSO2Event format via Thrift or Binary protocols. map-binary The siddhi-map-binary extension is an extension to Siddhi that can be used to convert binary events to/from Siddhi events. map-csv The siddhi-map-csv extension is an extension to Siddhi that supports mapping incoming events with csv format to a stream at the source, and mapping a stream to csv format events at the sink. map-json The siddhi-map-json extension is an extension to Siddhi which is used to convert JSON message to/from Siddhi events. map-keyvalue The siddhi-map-keyvalue extension is an extension to Siddhi that supports mapping incoming events with Key-Value map format to a stream at the source, and mapping a stream to Key-Value map events at the sink. map-text The siddhi-map-text extension is an extension to Siddhi that supports mapping incoming text messages to a stream at the source, and mapping a stream to text messages at the sink. map-wso2event The siddhi-map-wso2event extension is an extension to Siddhi that can be used to convert WSO2 events to/from Siddhi events. map-xml The siddhi-map-xml extension is an extension to Siddhi that supports mapping incoming XML events to a stream at the source, and mapping a stream to XML events at the sink. script-js The siddhi-script-js is an extension to Siddhi that allows to include JavaScript functions within the Siddhi Query Language. store-cassandra The siddhi-store-cassandra extension is an extension to Siddhi that can be used to persist events to a Cassandra instance of the users choice. Find some useful links below: store-hbase The siddhi-store-hbase extension is an extension to Siddhi that can be used to persist events to a HBase instance of the users choice. Find some useful links below: store-mongodb The siddhi-store-mongodb extension is an extension to Siddhi that can be used to persist events to a MongoDB instance of the users choice. Find some useful links below: store-rdbms The siddhi-store-rdbms extension is an extension to Siddhi that can be used to persist events to an RDBMS instance of the user's choice. store-redis The siddhi-store-redis extension is an extension for siddhi redis event table implementation. This extension can be used to persist events to a redis cloud instance of version 4.x.x. store-solr The siddhi-store-solr extension is an extension for siddhi Solr event table implementation. This extension can be used to persist events to a Solr cloud instance of version 6.x.x. Extensions released under GPL License Name Description execution-geo The siddhi-gpl-execution-geo extension is an extension to Siddhi that provides geo data related functionality such as checking whether a given geo coordinate is within a predefined geo-fence, etc. execution-nlp The siddhi-gpl-execution-nlp extension is an extension to Siddhi that can be used to process natural language. execution-pmml The siddhi-gpl-execution-pmml extension is an extension to Siddhi that can be used to process texts based on a PMML processing model. execution-r The siddhi-gpl-execution-r extension is an extension to Siddhi that can be used to process events with R scripts. execution-streamingml The siddhi-execution-streamingml is an extension to Siddhi that performs streaming machine learning on event streams. Extension Repositories All the extension repositories maintained by WSO2 can be found here","title":"Extension Guide"},{"location":"documentation/extensions/#siddhi-extensions","text":"","title":"Siddhi Extensions"},{"location":"documentation/extensions/#available-extensions","text":"Following are some pre-written extensions that are supported with Siddhi;","title":"Available Extensions"},{"location":"documentation/extensions/#extensions-released-under-apache-20-license","text":"Name Description execution-approximate The siddhi-execution-approximate is an extension to Siddhi that performs approximate computing on event streams. execution-env The siddhi-execution-env extension is an extension to Siddhi that provides the capability to read environment properties inside Siddhi stream definitions and use it inside queries. Functions of the env extension are as follows.. execution-extrema The siddhi-execution-extrema extension is an extension to Siddhi that processes event streams based on different arithmetic properties. Different types of processors are available to extract the extremas from the event streams according to the specified attribute in the stream. execution-geo The siddhi-execution-geo extension is an extension to Siddhi that provides geo data related functionality such as checking whether a given geo coordinate is within a predefined geo-fence, etc. Following are the functions of the Geo extension. execution-graph The siddhi-execution-graph extension is an extension to Siddhi that provides graph related functionality to Siddhi such as getting size of largest connected component of a graph, maximum clique size of a graph, etc. execution-kalmanfilter The siddhi-execution-kalman-filter extension is an extension to Siddhi provides that Kalman filtering capabilities to Siddhi. This allows you to detect outliers of input data. execution-map The siddhi-execution-map extension is an extension to Siddhi that provides the capability to send a map object inside Siddhi stream definitions and use it inside queries. The following are the functions of the map extension.. execution-markov The siddhi-execution-markov extension is an extension to Siddhi that allows abnormal patterns relating to user activity to be detected when carrying out real time analysis. execution-math The siddhi-execution-math is an extension to Siddhi, which provides useful mathematical functions that can make your siddhi queries more flexible. execution-priority The siddhi-execution-priority extension is an extension to Siddhi that keeps track of the priority of events in a stream. execution-regex The siddhi-execution-regex extension is an extension to Siddhi that provides basic RegEx execution capabilities. execution-reorder The siddhi-execution-reorder extension is an extension to Siddhi that is used for reordering events from an unordered event stream. Reorder extension is implemented using the K-Slack and alpha K-Stack algorithms. execution-sentiment The siddhi-execution-sentiment extension is an extension to Siddhi that performs sentiment analysis using Afinn Wordlist-based approach. execution-stats The siddhi-execution-stats extension is an extension to Siddhi that provides statistical functions for aggregated events. Currently this contains median function which calculate the median of aggregated events. Calculation of median is done for each event arrival and expiry, it is not recommended to use this extension for large window sizes. execution-streamingml The siddhi-execution-streamingml is an extension to Siddhi that performs streaming machine learning on event streams. execution-string The siddhi-execution-string is an extension to Siddhi that provides basic string handling capabilities such as con-cat, length, convert to lowercase, and replace all. execution-tensorflow The siddhi-execution-tensorflow is an extension to Siddhi that adds support for inferences from pre-built TensorFlow SavedModels using Siddhi. execution-time The siddhi-execution-time extension is an extension to Siddhi that provides time related functionality to Siddhi such as getting current time, current date, manipulating/formatting dates and etc. execution-timeseries The siddhi-execution-timeseries extension is an extension to Siddhi which enables users to forecast and detect outliers in time series data, using Linear Regression Models. execution-unique The siddhi-execution-unique extension is an extension to Siddhi that processes event streams based on unique events. Different types of unique windows are available to hold unique events based on the given unique key parameter. execution-unitconversion The siddhi-execution-unitconversion extension is an extension to Siddhi that enables conversions of length, mass, time and volume units. io-cdc The siddhi-io-cdc extension is an extension to Siddhi. It receives change data from MySQL, MS SQL Server, Postgresql, H2 and Oracle in the key-value format. io-email The siddhi-io-email extension is an extension to Siddhi that receives and publishes events via email. Using the extension, events can be published through smtp mail server and received through 'pop3' or 'imap' mail serves. io-file The siddhi-io-file extension is an extension to Siddhi which is used to receive/publish event data from/to file. It supports both binary and text formats. io-http The siddhi-io-http extension is an extension to Siddhi that allows you to receive and publish events via http and https transports and also allow you perform synchronous request and response. This extension works with WSO2 Stream Processor and with standalone Siddhi. io-jms The siddhi-io-jms extension is an extension to Siddhi that used to receive and publishe events via JMS Message. This extension allows users to subscribe to a JMS broker and receive/publish JMS messages. io-kafka The siddhi-io-kafka extension is an extension to Siddhi. This implements siddhi kafka source and sink that can be used to receive events from a kafka cluster and to publish events to a kafka cluster. io-mqtt The siddhi-io-mqtt is an extension to Siddhi mqtt source and sink implementation,that publish and receive events from mqtt broker. io-prometheus The siddhi-io-prometheus extension is an extension to Siddhi. The Prometheus-sink publishes Siddhi events as Prometheus metrics and expose them to Prometheus server. The Prometheus-source retrieves Prometheus metrics from an endpoint and send them as Siddhi events. io-rabbitmq The siddhi-io-rabbitmq is an extension to Siddhi that publish and receive events from rabbitmq broker. io-sqs The siddhi-io-sqs extension is an extension to Siddhi that used to receive and publish events via AWS SQS Service. This extension allows users to subscribe to a SQS queue and receive/publish SQS messages. io-tcp The siddhi-io-tcp extension is an extension to Siddhi that allows to receive and publish events through TCP. io-twitter The siddhi-io-twitter extension is an extension to Siddhi. It publishes event data from Twitter Applications in the key-value map format. io-websocket The siddhi-io-websocket extension is an extension to Siddhi that allows to receive and publish events through WebSocket. io-wso2event The siddhi-io-wso2event extension is an extension to Siddhi that receives and publishes events in the WSO2Event format via Thrift or Binary protocols. map-binary The siddhi-map-binary extension is an extension to Siddhi that can be used to convert binary events to/from Siddhi events. map-csv The siddhi-map-csv extension is an extension to Siddhi that supports mapping incoming events with csv format to a stream at the source, and mapping a stream to csv format events at the sink. map-json The siddhi-map-json extension is an extension to Siddhi which is used to convert JSON message to/from Siddhi events. map-keyvalue The siddhi-map-keyvalue extension is an extension to Siddhi that supports mapping incoming events with Key-Value map format to a stream at the source, and mapping a stream to Key-Value map events at the sink. map-text The siddhi-map-text extension is an extension to Siddhi that supports mapping incoming text messages to a stream at the source, and mapping a stream to text messages at the sink. map-wso2event The siddhi-map-wso2event extension is an extension to Siddhi that can be used to convert WSO2 events to/from Siddhi events. map-xml The siddhi-map-xml extension is an extension to Siddhi that supports mapping incoming XML events to a stream at the source, and mapping a stream to XML events at the sink. script-js The siddhi-script-js is an extension to Siddhi that allows to include JavaScript functions within the Siddhi Query Language. store-cassandra The siddhi-store-cassandra extension is an extension to Siddhi that can be used to persist events to a Cassandra instance of the users choice. Find some useful links below: store-hbase The siddhi-store-hbase extension is an extension to Siddhi that can be used to persist events to a HBase instance of the users choice. Find some useful links below: store-mongodb The siddhi-store-mongodb extension is an extension to Siddhi that can be used to persist events to a MongoDB instance of the users choice. Find some useful links below: store-rdbms The siddhi-store-rdbms extension is an extension to Siddhi that can be used to persist events to an RDBMS instance of the user's choice. store-redis The siddhi-store-redis extension is an extension for siddhi redis event table implementation. This extension can be used to persist events to a redis cloud instance of version 4.x.x. store-solr The siddhi-store-solr extension is an extension for siddhi Solr event table implementation. This extension can be used to persist events to a Solr cloud instance of version 6.x.x.","title":"Extensions released under Apache 2.0 License"},{"location":"documentation/extensions/#extensions-released-under-gpl-license","text":"Name Description execution-geo The siddhi-gpl-execution-geo extension is an extension to Siddhi that provides geo data related functionality such as checking whether a given geo coordinate is within a predefined geo-fence, etc. execution-nlp The siddhi-gpl-execution-nlp extension is an extension to Siddhi that can be used to process natural language. execution-pmml The siddhi-gpl-execution-pmml extension is an extension to Siddhi that can be used to process texts based on a PMML processing model. execution-r The siddhi-gpl-execution-r extension is an extension to Siddhi that can be used to process events with R scripts. execution-streamingml The siddhi-execution-streamingml is an extension to Siddhi that performs streaming machine learning on event streams.","title":"Extensions released under GPL License"},{"location":"documentation/extensions/#extension-repositories","text":"All the extension repositories maintained by WSO2 can be found here","title":"Extension Repositories"},{"location":"documentation/features/","text":"Features Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"documentation/features/#features","text":"Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"documentation/quckstart-5.x/","text":"Siddhi 5.x Quick Start Guide Siddhi is a 100% open source Streaming engine that listens to events from various data streams, detects complex conditions described via a Streaming SQL language, and triggers actions. It's highly optimized for performance and performs stream processing, complex event processing, streaming data integration, and adaptive intelligence for mission-critical systems. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Siddhi \u2018Hello World!\u2019 Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore Siddhi can play a vital part in any system built using event-driven architecture. As Siddhi works with events first let's understand what an event is through an example. If we consider the transactions carried out via an ATM as a data stream, one withdrawal from it can be considered an event . This event contains data about the amount, time, account number etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming data analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201devent data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming data integration Streaming data integration is way of integrating several systems by continuously moving data in realtime from one system to another, while processing, correlating, and analyzing the data in-memory. Adaptive Intelligence Decision Engine Basically, Siddhi receives data event-by-event and processes them in real time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture As indicated above, Siddhi can: Accept event inputs from many different types of sources Process them to generate insights Publish them to many types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . When the Siddhi application is started, it: Consumes data one-by-one as events Processes the data in each event Generates new high level events based on the processing done so far Sends newly generated events as the output to streams. 3. Using Siddhi for the first time In this section, we will be using the Tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated editor with a GUI(referred to as \u201cSiddhi Editor\u201d ) where you can write your query and simulate events as a data stream. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux) For Windows: tooling.bat For Linux: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser. http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Siddhi \u2018Hello World!\u2019 Siddhi Streaming SQL is a rich, compact, easy-to-learn SQL-like language. Let's first learn how to find the total of values coming into a data stream and output the current running total value with each event. Siddhi has lot of in-built functions and extensions available for complex analysis, but to get started, let's use a simple one. You can find more information about the Siddhi grammar and its functions in the Siddhi Query Guide . Let's consider a scenario where we are loading cargo boxes into a ship . We need to keep track of the total weight of the cargo added. Measuring the weight of a cargo box when loading is considered an event . We can write a Siddhi program for the above scenario which has 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This is a Siddhi routine. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name( HelloWorldApp ) Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. Also we need to attach a_\"source\"_ to the created stream so that we can send events to that stream. ( Source is the Siddhi way to consume streams from external systems. This particular http type source will spin up a HTTP endpoint and keep on listening to events though that endpoint. To learn more about sources, see source ) In this example: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data type - JSON @source(type = http , receiver.url = http://0.0.0.0:8006/cargo ,@map(type = json )) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the previous definition with an additional totalWeight attribute that contains the total weight calculated so far. Here, we need to add a \"sink\" to log the OutputStream so that we can observe the output values. ( Sink is the Siddhi way to publish streams to external systems. This particular log type sink just logs the stream events. To learn more about sinks, see sink ) @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aThe actual Siddhi query. Here we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d Which stream should be taken into processing\u200a\u2014\u200a \u201cCargoStream\u201d What data we require in the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d How the output should be calculated - by calculating the sum of the the *weight*s Which stream should be populated with the output\u200a\u2014\u200a \u201cOutputStream\u201d @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; Final Siddhi application in the editor will look like below. You can copy the final Siddhi app from below. 5. Testing Siddhi Application In this section we will be testing the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end integration test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. You should save your HelloWorldApp by browsing to File - Save before you run event simulation. Then click Event Simulator and configure it as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart\u201d . This starts the Siddhi Application. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: \u201cHelloWorldApp.siddhi Started Successfully!\u201d Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal You can see a log that contains \u201coutputData=[2, 2]\u201d . Click Send again and observe a log with \u201coutputData=[2, 4]\u201d . You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed creating Siddhi Hello World! 6. A bit of Stream Processing This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we have been carrying out the processing by having only the running sum value in-memory. No events were stored during this process. Window processing is a method that allows us to store some events in-memory for a given period so that we can perform operations such as calculating the average, maximum, etc values within them. Let's imagine that when we are loading cargo boxes into the ship we need to keep track of the average weight of the recently loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Here, we are specifying that the last 3 events should be kept in memory for processing. avg(weight) as averageWeight - Here, we are calculating the average of events stored in the window and producing the average value as \"averageWeight\" (Note: Now the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights of the last three cargo events. It is also notable that the defined length window only keeps 3 events in-memory. When the 4 th event arrives, the first event in the window is removed from memory. This ensures that the memory usage does not grow beyond a specific limit. There are also other implementations done in Siddhi to reduce the memory consumption. For more information, see Siddhi Architecture . 7. Running streaming application as a microservice This step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow below steps. Install docker in your machine and start the daemon.( https://docs.docker.com/install/ ) Pull the latest siddhi-runner image by executing below command docker pull siddhiio/siddhi-runner-alpine:latest Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. Move downloaded Siddhi file( HelloWorldModifiedApp.siddhi ) to a desired location(/home/me/siddhi-apps) Execute below command to start a streaming microservice which runs above developed siddhi application. (Please pay attention to the siddhi app name as in the Quick Start Guide we have changed the modified Siddhi app name from that of the original app.) docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldModifiedApp.siddhi Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header Content-Type:application/json \\ -d { event :{ weight :2}} You will observe below messages in container logs. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Query Guide . Feel free to try out Siddhi and event simulation to understand Siddhi better. If you have questions please post them to the Stackoverflow with \"Siddhi\" tag.","title":"Siddhi 5.x Quick Start Guide"},{"location":"documentation/quckstart-5.x/#siddhi-5x-quick-start-guide","text":"Siddhi is a 100% open source Streaming engine that listens to events from various data streams, detects complex conditions described via a Streaming SQL language, and triggers actions. It's highly optimized for performance and performs stream processing, complex event processing, streaming data integration, and adaptive intelligence for mission-critical systems. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Siddhi \u2018Hello World!\u2019 Testing Siddhi Application A bit of Stream Processing","title":"Siddhi 5.x Quick Start Guide"},{"location":"documentation/quckstart-5.x/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore Siddhi can play a vital part in any system built using event-driven architecture. As Siddhi works with events first let's understand what an event is through an example. If we consider the transactions carried out via an ATM as a data stream, one withdrawal from it can be considered an event . This event contains data about the amount, time, account number etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming data analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201devent data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming data integration Streaming data integration is way of integrating several systems by continuously moving data in realtime from one system to another, while processing, correlating, and analyzing the data in-memory. Adaptive Intelligence Decision Engine Basically, Siddhi receives data event-by-event and processes them in real time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"documentation/quckstart-5.x/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources Process them to generate insights Publish them to many types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . When the Siddhi application is started, it: Consumes data one-by-one as events Processes the data in each event Generates new high level events based on the processing done so far Sends newly generated events as the output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"documentation/quckstart-5.x/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated editor with a GUI(referred to as \u201cSiddhi Editor\u201d ) where you can write your query and simulate events as a data stream. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to TOOLING_HOME /bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux) For Windows: tooling.bat For Linux: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser. http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"documentation/quckstart-5.x/#4-siddhi-hello-world","text":"Siddhi Streaming SQL is a rich, compact, easy-to-learn SQL-like language. Let's first learn how to find the total of values coming into a data stream and output the current running total value with each event. Siddhi has lot of in-built functions and extensions available for complex analysis, but to get started, let's use a simple one. You can find more information about the Siddhi grammar and its functions in the Siddhi Query Guide . Let's consider a scenario where we are loading cargo boxes into a ship . We need to keep track of the total weight of the cargo added. Measuring the weight of a cargo box when loading is considered an event . We can write a Siddhi program for the above scenario which has 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This is a Siddhi routine. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name( HelloWorldApp ) Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. Also we need to attach a_\"source\"_ to the created stream so that we can send events to that stream. ( Source is the Siddhi way to consume streams from external systems. This particular http type source will spin up a HTTP endpoint and keep on listening to events though that endpoint. To learn more about sources, see source ) In this example: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data type - JSON @source(type = http , receiver.url = http://0.0.0.0:8006/cargo ,@map(type = json )) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the previous definition with an additional totalWeight attribute that contains the total weight calculated so far. Here, we need to add a \"sink\" to log the OutputStream so that we can observe the output values. ( Sink is the Siddhi way to publish streams to external systems. This particular log type sink just logs the stream events. To learn more about sinks, see sink ) @sink(type= log , prefix= LOGGER ) define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aThe actual Siddhi query. Here we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d Which stream should be taken into processing\u200a\u2014\u200a \u201cCargoStream\u201d What data we require in the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d How the output should be calculated - by calculating the sum of the the *weight*s Which stream should be populated with the output\u200a\u2014\u200a \u201cOutputStream\u201d @info(name= HelloWorldQuery ) from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; Final Siddhi application in the editor will look like below. You can copy the final Siddhi app from below.","title":"4. Siddhi \u2018Hello World!\u2019"},{"location":"documentation/quckstart-5.x/#5-testing-siddhi-application","text":"In this section we will be testing the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end integration test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. You should save your HelloWorldApp by browsing to File - Save before you run event simulation. Then click Event Simulator and configure it as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart\u201d . This starts the Siddhi Application. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: \u201cHelloWorldApp.siddhi Started Successfully!\u201d Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal You can see a log that contains \u201coutputData=[2, 2]\u201d . Click Send again and observe a log with \u201coutputData=[2, 4]\u201d . You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed creating Siddhi Hello World!","title":"5. Testing Siddhi Application"},{"location":"documentation/quckstart-5.x/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we have been carrying out the processing by having only the running sum value in-memory. No events were stored during this process. Window processing is a method that allows us to store some events in-memory for a given period so that we can perform operations such as calculating the average, maximum, etc values within them. Let's imagine that when we are loading cargo boxes into the ship we need to keep track of the average weight of the recently loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name= HelloWorldQuery ) from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Here, we are specifying that the last 3 events should be kept in memory for processing. avg(weight) as averageWeight - Here, we are calculating the average of events stored in the window and producing the average value as \"averageWeight\" (Note: Now the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights of the last three cargo events. It is also notable that the defined length window only keeps 3 events in-memory. When the 4 th event arrives, the first event in the window is removed from memory. This ensures that the memory usage does not grow beyond a specific limit. There are also other implementations done in Siddhi to reduce the memory consumption. For more information, see Siddhi Architecture .","title":"6. A bit of Stream Processing"},{"location":"documentation/quckstart-5.x/#7-running-streaming-application-as-a-microservice","text":"This step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow below steps. Install docker in your machine and start the daemon.( https://docs.docker.com/install/ ) Pull the latest siddhi-runner image by executing below command docker pull siddhiio/siddhi-runner-alpine:latest Navigate to Siddhi Editor and choose File - Export File for download above Siddhi application as a file. Move downloaded Siddhi file( HelloWorldModifiedApp.siddhi ) to a desired location(/home/me/siddhi-apps) Execute below command to start a streaming microservice which runs above developed siddhi application. (Please pay attention to the siddhi app name as in the Quick Start Guide we have changed the modified Siddhi app name from that of the original app.) docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldModifiedApp.siddhi Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header Content-Type:application/json \\ -d { event :{ weight :2}} You will observe below messages in container logs. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Query Guide . Feel free to try out Siddhi and event simulation to understand Siddhi better. If you have questions please post them to the Stackoverflow with \"Siddhi\" tag.","title":"7. Running streaming application as a microservice"},{"location":"documentation/query-guide-5.x/","text":"Siddhi 5.x Streaming SQL Guide Info Please find the Siddhi 4.x Streaming SQL Guide here Introduction Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL. Siddhi Application The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @ app : name ( Temperature-Analytics ) define stream TempStream ( deviceID long , roomNo int , temp double ); @ name ( 5minAvgQuery ) from TempStream # window . time ( 5 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo insert into OutputStream ; Stream A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream ( deviceID long , roomNo int , temp double ); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @ source ( type = source type , static . key = value , static . key = value , @ map ( type = map type , static . key = value , static . key = value , @ attributes ( attribute1 = attribute mapping , attributeN = attribute mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. WSO2Event Expose a Thrift and TCP services to consume events formatted as WSO2Events. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. RabbitMQ Subscribe to RabbitMQ topic to consume events. MQTT Subscribe to MQTT brokers to consume events. WebSocket Create a web-socket connection or expose a service to consume messages. Twitter Subscribe to Twitter to consume tweets. Amazon SQS Subscribe to Amazon SQS to consume events. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. WSO2Event Converts WSO2Events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { name : Paul , age : 20 , country : UK } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json )) define stream InputStream ( name string , age int , country string ); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { portfolio :{ stock :{ volume : 100 , company :{ symbol : FB }, price : 55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( symbol = stock.company.symbol , price = stock.price , volume = stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); The same can also be configured by omitting the attribute names as bellow. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( stock.company.symbol , stock.price , stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); Sink Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @ sink ( type = sink type , static . key = value , dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. WSO2Event Publish WSO2Events via Thrift or TCP protocols. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. RabbitMQ Publish events to RabbitMQ topics. MQTT Publish events to MQTT topics. WebSocket Create a web-socket connection or expose a service to publish messages. Amazon SQS Publish events to Amazon SQS. Prometheus Expose data through Prometheus agent. Distributed Sink Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = roundRobin , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = partitioned , partitionKey = partition key , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Sink Mapper Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. WSO2Event Converts Siddhi events to WSO2Event events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @ sink ( type = http , publisher . url = http://localhost:8005/endpoint , method = POST , headers = Accept-Date:20/02/2017 , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json )) define stream OutputStream ( name string , age int , country string ); This will publish a JSON message on the following format: { event :{ name : Paul , age : 20 , country : UK } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @ sink ( type = http , publisher . url = http://localhost:8005/stocks , @ map ( type = json , validate . json = true , enclosing . element = $.Portfolio , @ payload ( { StockData :{ Symbol : {{ symbol }} , Price :{{price}} }} ))) define stream StockStream ( symbol string , price float , volume long ); This will publish a single event as the JSON message on the following format: { Portfolio :{ StockData :{ Symbol : GOOG , Price : 55.6 } } } This can also publish multiple events together as a JSON message on the following format: { Portfolio :[ { StockData :{ Symbol : GOOG , Price : 55.6 } }, { StockData :{ Symbol : FB , Price : 57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @ sink ( type = http , method = POST , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json ), @ distribution ( strategy = partitioned , partitionKey = country , @ destination ( publisher . url = http://localhost:8005/endpoint1 ), @ destination ( publisher . url = http://localhost:8006/endpoint2 ))) define stream OutputStream ( name string , age int , country string ); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { event :{ name : Paul , age : 20 , country : UK } } Error Handling Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @ OnError ( action = on error action ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @ OnError ( name = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream ! TempStream ( deviceID long , roomNo int , temp double , _error object ); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @ OnError ( name = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Error generation through a custom function `createError()` @ name ( error-generation ) from TempStream # custom : createError () insert into IgnoreStream1 ; -- Handling error by simply logging the event and error. @ name ( handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream2 ; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @ sink ( type = sink type , on . error = on error action , key = value , ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @ sink ( type = kafka , on . error = WAIT , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @ OnError ( name = STREAM ) @ sink ( type = kafka , on . error = STREAM , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Handling error by simply logging the event and error. @ name ( handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream ; Query Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @ name ( query name ) from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions. Value Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert ( roomNo , string ) as roomNo , maximum ( tempReading1 , tempReading2 ) as temp , UUID () as messageID insert into RoomTempStream ; Filter Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not needed. Syntax Filter conditions should be defined in square brackets next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream [( roomNo = 100 and roomNo 210 ) and temp 40 ] select roomNo , temp insert into HighTempStream ; Window Windows allow you to capture a subset of events based on a specific criterion from an input stream for calculation. Each input stream can only have a maximum of one window. Purpose To create subsets of events within a stream based on time duration, number of events, etc for processing. A window can operate in a sliding or tumbling (batch) manner. Syntax The #window prefix should be inserted next to the relevant stream in order to use a window. Note Filter condition can be applied both before and/or after the window Example If you want to identify the maximum temperature out of the last 10 events, you need to define a length window of 10 events. This window operates in a sliding mode where the following 3 subsets are calculated when a list of 12 events are received in a sequential order. Subset Event Range 1 1-10 2 2-11 3 3-12 The following query finds the maximum temperature out of last 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream # window . length ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; If you define the maximum temperature reading out of every 10 events, you need to define a lengthBatch window of 10 events. This window operates as a batch/tumbling mode where the following 3 subsets are calculated when a list of 30 events are received in a sequential order. Subset Event Range 1 1-10 2 11-20 3 21-30 The following query finds the maximum temperature out of every 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream # window . lengthBatch ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Note Similar operations can be done based on time via time windows and timeBatch windows and for others. Code segments such as #window.time(10 min) considers events that arrive during the last 10 minutes in a sliding manner, and the #window.timeBatch(2 min) considers events that arrive every 2 minutes in a tumbling manner. Following are some inbuilt windows shipped with Siddhi. For more window types, see execution extensions . time timeBatch batch timeLength length lengthBatch sort frequent lossyFrequent session cron externalTime externalTimeBatch delay Output event types Projection of the query depends on the output event types such as, current and expired event types. By default all queries produce current events and only queries with windows produce expired events when events expire from the window. You can specify whether the output of a query should be only current events, only expired events or both current and expired events. Note! Controlling the output event types does not alter the execution within the query, and it does not affect the accuracy of the query execution. The following keywords can be used with the output stream to manipulate output. Output event types Description current events Outputs events when incoming events arrive to be processed by the query. This is default when no specific output event type is specified. expired events Outputs events when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. The output event type keyword can be used between insert and into as shown in the following example. Example This query delays all events in a stream by 1 minute. from TempStream # window . time ( 1 min ) select * insert expired events into DelayedTempStream Aggregate function Aggregate functions perform aggregate calculations in the query. When a window is defined the aggregation is restricted within that window. If no window is provided aggregation is performed from the start of the Siddhi application. Syntax from input stream # window . window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Aggregate Parameters Aggregate parameters can be attributes, constant values, results of other functions or aggregates, results of mathematical or logical expressions, or time parameters. Aggregate parameters configured in a query depends on the aggregate function being called. Example The following query calculates the average value for the temp attribute of the TempStream stream. This calculation is done for the last 10 minutes in a sliding manner, and the result is output as avgTemp to the AvgTempStream output stream. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID insert into AvgTempStream ; Following are some inbuilt aggregation functions shipped with Siddhi, for more aggregation functions, see execution extensions . avg sum max min count distinctCount maxForever minForever stdDev Group By Group By allows you to group the aggregate based on specified attributes. Syntax The syntax for the Group By aggregate function is as follows: from input stream # window . window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for a sliding time window of 10 minutes. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID insert into AvgTempStream ; Having Having allows you to filter events after processing the select statement. Purpose This allows you to filter the aggregation output. Syntax The syntax for the Having clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Example The following query calculates the average temperature per room for the last 10 minutes, and alerts if it exceeds 30 degrees. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo group by roomNo having avgTemp 30 insert into AlertStream ; Order By Order By allows you to order the aggregated result in ascending and/or descending order based on specified attributes. By default ordering will be done in ascending manner. User can use 'desc' keyword to order in descending manner. Syntax The syntax for the Order By clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination for every 10 minutes, and generate output events by ordering them in the ascending order of the room's avgTemp and then by the descending order of roomNo. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp , roomNo desc insert into AvgTempStream ; Limit Offset When events are emitted as a batch, offset allows you to offset beginning of the output event batch and limit allows you to limit the number of events in the batch from the defined offset. With this users can specify which set of events need be emitted. Syntax The syntax for the Limit Offset clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... limit positive interger ? offset positive interger ? insert into output stream ; Here both limit and offset are optional where limit by default output all the events and offset by default set to 0 . Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits two events with highest average temperature. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream ; The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits third, forth and fifth events when sorted in descending order based on their average temperature. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream ; Join (Stream) Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream # window . window name ( parameter , ... ) { unidirectional } { as reference } join input stream # window . window name ( parameter , ... ) { unidirectional } { as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , isOn bool ); from TempStream [ temp 30 . 0 ] # window . time ( 1 min ) as T join RegulatorStream [ isOn == false ] # window . length ( 1 ) as R on T . roomNo == R . roomNo select T . roomNo , R . deviceID , start as action insert into RegulatorActionStream ; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Pattern This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from ( every ) ? event reference = input stream [ filter condition ] - ( every ) ? event reference = input stream [ filter condition ] - ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream | Items| Description | |-------------------|-------------| | - | This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. | | event reference | This allows you to add a reference to the the matching event so that it can be accessed later for further processing. | | (within time gap )? | The within clause is optional. It defines the time duration within which all the matching events should occur. | | every | every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. | Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every ( e1 = TempStream ) - e2 = TempStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream . Counting Pattern Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from ( every ) ? event reference = input stream [ filter condition ] ( min count : max count ) ? - ... ( within time gap ) ? select event reference ([ event index ]) ? . attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); from every ( e1 = RegulatorStream ) - e2 = TempStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TempDiffStream ; Logical Patterns Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from ( every ) ? ( not ) ? event reference = input stream [ filter condition ] (( and | or ) event reference = input stream [ filter condition ]) ? ( within time gap ) ? - ... select event reference ([ event index ]) ? . attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point. Detecting Non-occurring Events Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); from every ( e1 = RegulatorStateChangeStream [ action == on ] ) - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp 12 ] and e2 = RegulatorStateChangeStream [ action == off ] select e1 . roomNo as roomNo insert into AlertStream ; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp 12 ] for 5 min select e1 . roomNo as roomNo insert into AlertStream ; Sequence Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from ( every ) ? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1 = TempStream , e2 = TempStream [ e1 . temp + 1 temp ] select e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from ( every ) ? event reference = input stream [ filter condition ]( +|*|? ) ? , event reference = input stream [ filter condition ]( +|*|? ) ? , ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream ( deviceID long , roomNo int , temp double ); from every e1 = TempStream , e2 = TempStream [ e1 . temp = temp ] + , e3 = TempStream [ e2 [ last ]. temp temp ] select e1 . temp as initialTemp , e2 [ last ]. temp as peakTemp insert into PeekTempStream ; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from ( every ) ? ( not ) ? event reference = input stream [ filter condition ] (( and | or ) event reference = input stream [ filter condition ]) ? ( within time gap ) ? , ... select event reference ([ event index ]) ? . attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream ( deviceID long , temp double ); define stream HumidStream ( deviceID long , humid double ); define stream RegulatorStream ( deviceID long , isOn bool ); from every e1 = RegulatorStream , e2 = TempStream and e3 = HumidStream select e2 . temp , e3 . humid insert into StateNotificationStream ; Output rate limiting Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Partition Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Inner Stream Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Purge Partition Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @ purge ( enable = true , interval = purge interval , idle . period = idle period of partition instance ) partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end ; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @ purge ( enable = true , interval = 1 sec , idle . period = 15 sec ) partition with ( deviceID of TempStream ) begin from TempStream # window . lengthBatch ( 10 ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into # AvgTempStream from every ( e1 =# AvgTempStream ), e2 =# AvgTempStream [ e1 . avgTemp + 5 avgTemp ] select e1 . deviceID , e1 . avgTemp as initialAvgTemp , e2 . avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end ; Table A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int , type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @ PrimaryKey ( symbol ) define table StockTable ( symbol string , price float , volume long ); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @ Index ( roomNo ) define table RoomTypeTable ( roomNo int , type string ); Store Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN ) define table TableName ( attribute1 Type1 , attributeN TypeN ); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @ Store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/hotel , username = siddhi , password = 123 , jdbc . driver . name = com.mysql.jdbc.Driver ) define table RoomTypeTable ( roomNo int , type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) Solr MongoDB HBase Redis Cassandra Operators on Table (and Store) The following operators can be performed on tables (and stores). Insert This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific output event types. For more information, see output event type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable ; Join (Table) This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable ( roomNo int , type string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream join RoomTypeTable on RoomTypeTable . roomNo == TempStream . roomNo select deviceID , RoomTypeTable . type as roomType , type , temp having roomType == server-room insert into ServerRoomTempStream ; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table. Delete To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table ( for output event type ) ? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific output event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable ( roomNo int , type string ); define stream DeleteStream ( roomNumber int ); from DeleteStream delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; Update This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table ( for output event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable ( roomNo int , people int ); define stream UpdateStream ( roomNumber int , arrival int , exit int ); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable . people = RoomOccupancyTable . people + arrival - exit on RoomOccupancyTable . roomNo == roomNumber ; Update or Insert This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table ( for output event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see output event type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable ( roomNo int , type string , assignee string ); define stream RoomAssigneeStream ( roomNumber int , type string , assignee string ); from RoomAssigneeStream select roomNumber as roomNo , type , assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ; In This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable ( roomNo int ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream [ ServerRoomTable . roomNo == roomNo in ServerRoomTable ] insert into ServerRoomTempStream ; Named Aggregation Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @ store ( type = store type , ...) @ purge ( enable = true or false , interval = purging interval , @ retentionPeriod ( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: ' @purge (enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); @ purge ( enable = true , interval = 10 sec , @ retentionPeriod ( sec = 120 sec , min = 24 hours , hours = 30 days , days = 1 year , months = all , years = all )) define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; Distributed Aggregation Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database( @store ). Syntax @ store ( type = store type , ...) @ PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency. Join (Aggregation) This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP , symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 **:**:** +05:30 per hours select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream ( symbol string , value int , perValue string ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 1496200000000 L , 1596434876000 L per S . perValue select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation. Named Window A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026 ) output event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see output event type . Examples Returning all output when events arrive and when events expire from the window. In this query, the output event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ); + Returning an output only when events expire from the window. In this query, the output event type of the window is `expired events`. Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ) output expired events ; Operators on Named Windows The following operators can be performed on named windows. Insert This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific output event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see output event type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream ( tempId string , temp double ); define window OneMinTempWindow ( tempId string , temp double ) time ( 1 min ); from TempStream select * insert into OneMinTempWindow ; Join (Window) To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow ( roomNo int , temp double ) time ( 2 min ); define stream CheckStream ( requestId string ); from CheckStream as C join TwoMinTempWindow as T on T . temp 40 select requestId , count ( T . temp ) as count insert into HighTempCountStream ; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window. From A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow ( roomNo int , temp double ) time ( 5 min ); from FiveMinTempWindow select max ( temp ) as maxValue , roomNo insert into MaxSensorReadingStream ; Trigger Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ( start | every time interval | cron expression ); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name ( triggered_time long ); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min ; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at 0 15 10 ? * MON-FRI ; Script Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function } ; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn [ javascript ] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3 ; return responce ; } ; define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select concatFn ( roomNo , - , deviceID ) as id , temp insert into DeviceTempStream ; Store Query Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime . query ( store query ); (Table/Window) Select The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table / window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10 ; select roomNo , type (Aggregation) Select The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select symbol , total , avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == FB within 2014-02-15 **:**:** +05:30 per hours select symbol , total , avgPrice ; Insert This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo , 2 as people insert into RoomOccupancyTable Delete The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; delete RoomTypeTable on RoomTypeTable . roomNo == 10 ; Update The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ... ? update table set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber , 1 as arrival update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + arrival on RoomTypeTable . roomNo == roomNumber ; update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + 1 on RoomTypeTable . roomNo == 10 ; Update or Insert This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo , single as type , abc as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ; Extensions Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream [ price = 20 ] # window . foo : unique ( symbol ) select symbol , price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further. Writing Custom Extensions Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N {_nameSpaceOfFunction} artifactId Artifact Id of the project N siddhi-execution-{_nameSpaceOfFunction} classNameOfAggregateFunction Class name of the Aggregate Function N ${_nameOfFunction}AggregateFunction classNameOfFunction Class name of the Function N ${_nameOfFunction}Function classNameOfStreamFunction Class name of the Stream Function N ${_nameOfFunction}StreamFunction classNameOfStreamProcessor Class name of the Stream Processor N ${_nameOfFunction}StreamProcessor classNameOfWindow Class name of the Window N ${_nameOfFunction}Window To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N {_IOType} artifactId Artifact Id of the project N siddhi-io-{_IOType} classNameOfSink Class name of the Sink N {_IOType}Sink classNameOfSource Class name of the Source N {_IOType}Source To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N {_mapType} artifactId Artifact Id of the project N siddhi-map-{_mapType} classNameOfSinkMapper Class name of the Sink Mapper N {_mapType}SinkMapper classNameOfSourceMapper Class name of the Source Mapper N {_mapType}SourceMapper To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N {_nameOfScript} artifactId Artifact Id of the project N siddhi-script-{_nameOfScript} classNameOfScript Class name of the Script N Eval{_nameOfScript} To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N {_storeType} artifactId Artifact Id of the project N siddhi-store-{_storeType} className Class name of the Store N {_storeType}EventTable To confirm that all property values are correct, type Y in the console. If not, press N . Configuring and Monitoring Siddhi Applications Multi-threading and Asynchronous Processing When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @ Async ( buffer . size = 256 , workers = 2 , batch . size . max = 5 ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size Statistics Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @ app : statistics ( reporter = console ) The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @ App : name ( TestMetrics ) @ App : Statistics ( reporter = console ) define stream TestStream ( message string ); @ info ( name = logQuery ) from TestSream # log ( Message: ) insert into TempSream ; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Query Guide"},{"location":"documentation/query-guide-5.x/#siddhi-5x-streaming-sql-guide","text":"Info Please find the Siddhi 4.x Streaming SQL Guide here","title":"Siddhi 5.x Streaming SQL Guide"},{"location":"documentation/query-guide-5.x/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"documentation/query-guide-5.x/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name(' name ') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. siddhi app : app annotation * ( stream definition | table definition | ... ) + ( query | partition ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @ app : name ( Temperature-Analytics ) define stream TempStream ( deviceID long , roomNo int , temp double ); @ name ( 5minAvgQuery ) from TempStream # window . time ( 5 min ) select roomNo , avg ( temp ) as avgTemp group by roomNo insert into OutputStream ;","title":"Siddhi Application"},{"location":"documentation/query-guide-5.x/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream ( deviceID long , roomNo int , temp double ); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"documentation/query-guide-5.x/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @ source ( type = source type , static . key = value , static . key = value , @ map ( type = map type , static . key = value , static . key = value , @ attributes ( attribute1 = attribute mapping , attributeN = attribute mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. WSO2Event Expose a Thrift and TCP services to consume events formatted as WSO2Events. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. RabbitMQ Subscribe to RabbitMQ topic to consume events. MQTT Subscribe to MQTT brokers to consume events. WebSocket Create a web-socket connection or expose a service to consume messages. Twitter Subscribe to Twitter to consume tweets. Amazon SQS Subscribe to Amazon SQS to consume events. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"documentation/query-guide-5.x/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( attribute1 =' mapping ', attributeN =' mapping ') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( ' mapping for attribute1 ', ' mapping for attributeN ') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. WSO2Event Converts WSO2Events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { name : Paul , age : 20 , country : UK } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json )) define stream InputStream ( name string , age int , country string ); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { portfolio :{ stock :{ volume : 100 , company :{ symbol : FB }, price : 55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( symbol = stock.company.symbol , price = stock.price , volume = stock.volume ))) define stream StockStream ( symbol string , price float , volume long ); The same can also be configured by omitting the attribute names as bellow. @ source ( type = http , receiver . url = http://0.0.0.0:8080/foo , @ map ( type = json , enclosing . element = $.portfolio , @ attributes ( stock.company.symbol , stock.price , stock.volume ))) define stream StockStream ( symbol string , price float , volume long );","title":"Source Mapper"},{"location":"documentation/query-guide-5.x/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @ sink ( type = sink type , static . key = value , dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) ) define stream stream name ( attribute1 type , attributeN type ); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. WSO2Event Publish WSO2Events via Thrift or TCP protocols. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. RabbitMQ Publish events to RabbitMQ topics. MQTT Publish events to MQTT topics. WebSocket Create a web-socket connection or expose a service to publish messages. Amazon SQS Publish events to Amazon SQS. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"documentation/query-guide-5.x/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = roundRobin , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type ); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @ sink ( type = sink type , common . static . key = value , common . dynamic . key = {{ value }} , @ map ( type = map type , static . key = value , dynamic . key = {{ value }} , @ payload ( payload mapping ) ) @ distribution ( strategy = partitioned , partitionKey = partition key , @ destination ( destination . specific . key = value ), @ destination ( destination . specific . key = value ))) ) define stream stream name ( attribute1 type , attributeN type );","title":"Distributed Sink"},{"location":"documentation/query-guide-5.x/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. WSO2Event Converts Siddhi events to WSO2Event events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @ sink ( type = http , publisher . url = http://localhost:8005/endpoint , method = POST , headers = Accept-Date:20/02/2017 , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json )) define stream OutputStream ( name string , age int , country string ); This will publish a JSON message on the following format: { event :{ name : Paul , age : 20 , country : UK } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @ sink ( type = http , publisher . url = http://localhost:8005/stocks , @ map ( type = json , validate . json = true , enclosing . element = $.Portfolio , @ payload ( { StockData :{ Symbol : {{ symbol }} , Price :{{price}} }} ))) define stream StockStream ( symbol string , price float , volume long ); This will publish a single event as the JSON message on the following format: { Portfolio :{ StockData :{ Symbol : GOOG , Price : 55.6 } } } This can also publish multiple events together as a JSON message on the following format: { Portfolio :[ { StockData :{ Symbol : GOOG , Price : 55.6 } }, { StockData :{ Symbol : FB , Price : 57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @ sink ( type = http , method = POST , basic . auth . enabled = true , basic . auth . username = admin , basic . auth . password = admin , @ map ( type = json ), @ distribution ( strategy = partitioned , partitionKey = country , @ destination ( publisher . url = http://localhost:8005/endpoint1 ), @ destination ( publisher . url = http://localhost:8006/endpoint2 ))) define stream OutputStream ( name string , age int , country string ); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { event :{ name : Paul , age : 20 , country : UK } }","title":"Sink Mapper"},{"location":"documentation/query-guide-5.x/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @ OnError ( action = on error action ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as ! stream name . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @ OnError ( name = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream ! TempStream ( deviceID long , roomNo int , temp double , _error object ); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @ OnError ( name = STREAM ) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Error generation through a custom function `createError()` @ name ( error-generation ) from TempStream # custom : createError () insert into IgnoreStream1 ; -- Handling error by simply logging the event and error. @ name ( handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream2 ; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @ sink ( type = sink type , on . error = on error action , key = value , ...) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @ sink ( type = kafka , on . error = WAIT , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @ OnError ( name = STREAM ) @ sink ( type = kafka , on . error = STREAM , topic = {{roomNo}} , bootstrap . servers = localhost:9092 , @ map ( type = xml )) define stream TempStream ( deviceID long , roomNo int , temp double ); -- Handling error by simply logging the event and error. @ name ( handle-error ) from ! TempStream # log ( Error Occurred! ) select deviceID , roomNo , temp , _error insert into IgnoreStream ;","title":"Error Handling"},{"location":"documentation/query-guide-5.x/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @ name ( query name ) from input projection output action The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name(' query name ') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions.","title":"Query"},{"location":"documentation/query-guide-5.x/#value","text":"Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int + 123 , -75 , +95 long +L 123000L , -750l , +154L float ( +)?('.' *)? (E(-|+)? +)?F 123.0f , -75.0e-10F , +95.789f double ( +)?('.' *)? (E(-|+)? +)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '( char * !('|\"|\"\"\"| line ))' or \"( char * !(\"|\"\"\"| line ))\" or \"\"\"( char * !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format ( digit + unit )+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"documentation/query-guide-5.x/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"documentation/query-guide-5.x/#function","text":"Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, function name ( parameter * ) Here function name uniquely identifies the function. The parameter defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert ( roomNo , string ) as roomNo , maximum ( tempReading1 , tempReading2 ) as temp , UUID () as messageID insert into RoomTempStream ;","title":"Function"},{"location":"documentation/query-guide-5.x/#filter","text":"Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to pass through if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not needed. Syntax Filter conditions should be defined in square brackets next to the input stream as shown below. from input stream [ filter condition ] select attribute name , attribute name , ... insert into output stream Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream [( roomNo = 100 and roomNo 210 ) and temp 40 ] select roomNo , temp insert into HighTempStream ;","title":"Filter"},{"location":"documentation/query-guide-5.x/#window","text":"Windows allow you to capture a subset of events based on a specific criterion from an input stream for calculation. Each input stream can only have a maximum of one window. Purpose To create subsets of events within a stream based on time duration, number of events, etc for processing. A window can operate in a sliding or tumbling (batch) manner. Syntax The #window prefix should be inserted next to the relevant stream in order to use a window. Note Filter condition can be applied both before and/or after the window Example If you want to identify the maximum temperature out of the last 10 events, you need to define a length window of 10 events. This window operates in a sliding mode where the following 3 subsets are calculated when a list of 12 events are received in a sequential order. Subset Event Range 1 1-10 2 2-11 3 3-12 The following query finds the maximum temperature out of last 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream # window . length ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; If you define the maximum temperature reading out of every 10 events, you need to define a lengthBatch window of 10 events. This window operates as a batch/tumbling mode where the following 3 subsets are calculated when a list of 30 events are received in a sequential order. Subset Event Range 1 1-10 2 11-20 3 21-30 The following query finds the maximum temperature out of every 10 events from the TempStream stream, and inserts the results into the MaxTempStream stream. from TempStream # window . lengthBatch ( 10 ) select max ( temp ) as maxTemp insert into MaxTempStream ; Note Similar operations can be done based on time via time windows and timeBatch windows and for others. Code segments such as #window.time(10 min) considers events that arrive during the last 10 minutes in a sliding manner, and the #window.timeBatch(2 min) considers events that arrive every 2 minutes in a tumbling manner. Following are some inbuilt windows shipped with Siddhi. For more window types, see execution extensions . time timeBatch batch timeLength length lengthBatch sort frequent lossyFrequent session cron externalTime externalTimeBatch delay Output event types Projection of the query depends on the output event types such as, current and expired event types. By default all queries produce current events and only queries with windows produce expired events when events expire from the window. You can specify whether the output of a query should be only current events, only expired events or both current and expired events. Note! Controlling the output event types does not alter the execution within the query, and it does not affect the accuracy of the query execution. The following keywords can be used with the output stream to manipulate output. Output event types Description current events Outputs events when incoming events arrive to be processed by the query. This is default when no specific output event type is specified. expired events Outputs events when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. The output event type keyword can be used between insert and into as shown in the following example. Example This query delays all events in a stream by 1 minute. from TempStream # window . time ( 1 min ) select * insert expired events into DelayedTempStream","title":"Window"},{"location":"documentation/query-guide-5.x/#aggregate-function","text":"Aggregate functions perform aggregate calculations in the query. When a window is defined the aggregation is restricted within that window. If no window is provided aggregation is performed from the start of the Siddhi application. Syntax from input stream # window . window name ( parameter , parameter , ... ) select aggregate function ( parameter , parameter , ... ) as attribute name , attribute2 name , ... insert into output stream ; Aggregate Parameters Aggregate parameters can be attributes, constant values, results of other functions or aggregates, results of mathematical or logical expressions, or time parameters. Aggregate parameters configured in a query depends on the aggregate function being called. Example The following query calculates the average value for the temp attribute of the TempStream stream. This calculation is done for the last 10 minutes in a sliding manner, and the result is output as avgTemp to the AvgTempStream output stream. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID insert into AvgTempStream ; Following are some inbuilt aggregation functions shipped with Siddhi, for more aggregation functions, see execution extensions . avg sum max min count distinctCount maxForever minForever stdDev","title":"Aggregate function"},{"location":"documentation/query-guide-5.x/#group-by","text":"Group By allows you to group the aggregate based on specified attributes. Syntax The syntax for the Group By aggregate function is as follows: from input stream # window . window name (...) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for a sliding time window of 10 minutes. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID insert into AvgTempStream ;","title":"Group By"},{"location":"documentation/query-guide-5.x/#having","text":"Having allows you to filter events after processing the select statement. Purpose This allows you to filter the aggregation output. Syntax The syntax for the Having clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition insert into output stream ; Example The following query calculates the average temperature per room for the last 10 minutes, and alerts if it exceeds 30 degrees. from TempStream # window . time ( 10 min ) select avg ( temp ) as avgTemp , roomNo group by roomNo having avgTemp 30 insert into AlertStream ;","title":"Having"},{"location":"documentation/query-guide-5.x/#order-by","text":"Order By allows you to order the aggregated result in ascending and/or descending order based on specified attributes. By default ordering will be done in ascending manner. User can use 'desc' keyword to order in descending manner. Syntax The syntax for the Order By clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... insert into output stream ; Example The following query calculates the average temperature per roomNo and deviceID combination for every 10 minutes, and generate output events by ordering them in the ascending order of the room's avgTemp and then by the descending order of roomNo. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp , roomNo desc insert into AvgTempStream ;","title":"Order By"},{"location":"documentation/query-guide-5.x/#limit-offset","text":"When events are emitted as a batch, offset allows you to offset beginning of the output event batch and limit allows you to limit the number of events in the batch from the defined offset. With this users can specify which set of events need be emitted. Syntax The syntax for the Limit Offset clause is as follows: from input stream # window . window name ( ... ) select aggregate function ( parameter , parameter , ...) as attribute1 name , attribute2 name , ... group by attribute1 name , attribute2 name ... having condition order by attribute1 name ( asc | desc ) ? , attribute2 name ( ascend / descend ) ? , ... limit positive interger ? offset positive interger ? insert into output stream ; Here both limit and offset are optional where limit by default output all the events and offset by default set to 0 . Example The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits two events with highest average temperature. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream ; The following query calculates the average temperature per roomNo and deviceID combination, for events that arrive at the TempStream stream for every 10 minutes and emits third, forth and fifth events when sorted in descending order based on their average temperature. from TempStream # window . timeBatch ( 10 min ) select avg ( temp ) as avgTemp , roomNo , deviceID group by roomNo , deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream ;","title":"Limit &amp; Offset"},{"location":"documentation/query-guide-5.x/#join-stream","text":"Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from input stream # window . window name ( parameter , ... ) { unidirectional } { as reference } join input stream # window . window name ( parameter , ... ) { unidirectional } { as reference } on join condition select attribute name , attribute name , ... insert into output stream Here, the join condition allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , isOn bool ); from TempStream [ temp 30 . 0 ] # window . time ( 1 min ) as T join RegulatorStream [ isOn == false ] # window . length ( 1 ) as R on T . roomNo == R . roomNo select T . roomNo , R . deviceID , start as action insert into RegulatorActionStream ; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ;","title":"Join (Stream)"},{"location":"documentation/query-guide-5.x/#pattern","text":"This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from ( every ) ? event reference = input stream [ filter condition ] - ( every ) ? event reference = input stream [ filter condition ] - ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream | Items| Description | |-------------------|-------------| | - | This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. | | event reference | This allows you to add a reference to the the matching event so that it can be accessed later for further processing. | | (within time gap )? | The within clause is optional. It defines the time duration within which all the matching events should occur. | | every | every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. | Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every ( e1 = TempStream ) - e2 = TempStream [ e1 . roomNo == roomNo and ( e1 . temp + 5 ) = temp ] within 10 min select e1 . roomNo , e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream .","title":"Pattern"},{"location":"documentation/query-guide-5.x/#counting-pattern","text":"Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from ( every ) ? event reference = input stream [ filter condition ] ( min count : max count ) ? - ... ( within time gap ) ? select event reference ([ event index ]) ? . attribute name , ... insert into output stream Postfix Description Example n1:n2 This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. n: This matches n or more events (including n ). 2: matches 2 or more events. :n This matches up to n events (excluding n ). :5 matches up to 5 events. n This matches exactly n events. 5 matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream ( deviceID long , roomNo int , temp double ); define stream RegulatorStream ( deviceID long , roomNo int , tempSet double , isOn bool ); from every ( e1 = RegulatorStream ) - e2 = TempStream [ e1 . roomNo == roomNo ] 1 : - e3 = RegulatorStream [ e1 . roomNo == roomNo ] select e1 . roomNo , e2 [ 0 ]. temp - e2 [ last ]. temp as tempDiff insert into TempDiffStream ;","title":"Counting Pattern"},{"location":"documentation/query-guide-5.x/#logical-patterns","text":"Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from ( every ) ? ( not ) ? event reference = input stream [ filter condition ] (( and | or ) event reference = input stream [ filter condition ]) ? ( within time gap ) ? - ... select event reference ([ event index ]) ? . attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not condition1 and condition2 When not is included with and , it identifies the events that match arriving before any event that match . not condition for time period When not is included with for , it allows you to identify a situation where no event that matches condition1 arrives during the specified time period . e.g., from not TemperatureStream[temp 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given time period . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point.","title":"Logical Patterns"},{"location":"documentation/query-guide-5.x/#detecting-non-occurring-events","text":"Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for time period The non-occurrence of event A within time period after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for time period and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for time period 1 and not B for time period 2 After system start up, event A doess not occur within time period 1 , and event B also does not occur within time period 2 . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for time period or B After system start up, either event A does not occur within time period , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for time period 1 or not B for time period 2 After system start up, either event A does not occur within time period 1 , or event B occurs within time period 2 . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for time period Event B does not occur within time period after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for time period and B After the occurrence of P*, event A does not occur within time period , and event B occurs at some point in time. P* \u2192 not A for time period 1 and not B for time period 2 After the occurrence of P*, event A does not occur within time period 1 , and event B does not occur within time period 2 . P* \u2192 not A for time period or B After the occurrence of P*, either event A does not occur within time period , or event B occurs at some point in time. P* \u2192 not A for time period 1 or not B for time period 2 After the occurrence of P*, either event A does not occur within time period 1 , or event B does not occur within time period 2 . not A for time period \u2192 B Event A does occur within time period after the system start up, but event B occurs after that time period has elapsed. not A for time period and B \u2192 P* Event A does not occur within time period , and event B occurs at some point in time. Then P* occurs after the time period has elapsed, and after B has occurred. not A for time period 1 and not B for time period 2 \u2192 P* After system start up, event A does not occur within time period 1 , and event B does not occur within time period 2 . However, P* occurs after both A and B. not A for time period or B \u2192 P* After system start up, event A does not occur within time period or event B occurs at some point in time. The P* occurs after time period has elapsed, or after B has occurred. not A for time period 1 or not B for time period 2 \u2192 P* After system start up, either event A does not occur within time period 1 , or event B does not occur within time period 2 . Then P* occurs after both time period 1 and time period 2 have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream RoomKeyStream ( deviceID long , roomNo int , action string ); from every ( e1 = RegulatorStateChangeStream [ action == on ] ) - e2 = RoomKeyStream [ e1 . roomNo == roomNo and action == removed ] or e3 = RegulatorStateChangeStream [ e1 . roomNo == roomNo and action == off ] select e1 . roomNo , ifThenElse ( e2 is null , none , stop ) as action having action != none insert into RegulatorActionStream ; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp 12 ] and e2 = RegulatorStateChangeStream [ action == off ] select e1 . roomNo as roomNo insert into AlertStream ; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream ( deviceID long , roomNo int , tempSet double , action string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from e1 = RegulatorStateChangeStream [ action == start ] - not TempStream [ e1 . roomNo == roomNo and temp 12 ] for 5 min select e1 . roomNo as roomNo insert into AlertStream ;","title":"Detecting Non-occurring Events"},{"location":"documentation/query-guide-5.x/#sequence","text":"Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from ( every ) ? event reference = input stream [ filter condition ], event reference = input stream [ filter condition ], ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. event reference This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within time gap )? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1 = TempStream , e2 = TempStream [ e1 . temp + 1 temp ] select e1 . temp as initialTemp , e2 . temp as finalTemp insert into AlertStream ; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from ( every ) ? event reference = input stream [ filter condition ]( +|*|? ) ? , event reference = input stream [ filter condition ]( +|*|? ) ? , ... ( within time gap ) ? select event reference . attribute name , event reference . attribute name , ... insert into output stream Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream ( deviceID long , roomNo int , temp double ); from every e1 = TempStream , e2 = TempStream [ e1 . temp = temp ] + , e3 = TempStream [ e2 [ last ]. temp temp ] select e1 . temp as initialTemp , e2 [ last ]. temp as peakTemp insert into PeekTempStream ; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from ( every ) ? ( not ) ? event reference = input stream [ filter condition ] (( and | or ) event reference = input stream [ filter condition ]) ? ( within time gap ) ? , ... select event reference ([ event index ]) ? . attribute name , ... insert into output stream Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream ( deviceID long , temp double ); define stream HumidStream ( deviceID long , humid double ); define stream RegulatorStream ( deviceID long , isOn bool ); from every e1 = RegulatorStream , e2 = TempStream and e3 = HumidStream select e2 . temp , e3 . humid insert into StateNotificationStream ;","title":"Sequence"},{"location":"documentation/query-guide-5.x/#output-rate-limiting","text":"Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from input stream ... select attribute name , attribute name , ... output rate limiting configuration insert into output stream Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time output event every time interval This outputs output event every time interval time interval. Based on number of events output event every event interval events This outputs output event for every event interval number of events. Snapshot based output snapshot every time interval This outputs all events in the window (or the last event if no window is defined in the query) for every given time interval time interval. Here the output event specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no output event is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream;","title":"Output rate limiting"},{"location":"documentation/query-guide-5.x/#partition","text":"Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( expression of stream name , expression of stream name , ... ) begin query query ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( condition as partition key or condition as partition key or ... of stream name , ... ) begin query query ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo = 1030 as 'serverRoom' or roomNo 1030 and roomNo = 330 as 'officeRoom' or roomNo 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end;","title":"Partition"},{"location":"documentation/query-guide-5.x/#inner-stream","text":"Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Inner Stream"},{"location":"documentation/query-guide-5.x/#purge-partition","text":"Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @ purge ( enable = true , interval = purge interval , idle . period = idle period of partition instance ) partition with ( partition key of input stream ) begin from input stream ... select attribute name , attribute name , ... insert into output stream end ; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @ purge ( enable = true , interval = 1 sec , idle . period = 15 sec ) partition with ( deviceID of TempStream ) begin from TempStream # window . lengthBatch ( 10 ) select roomNo , deviceID , avg ( temp ) as avgTemp insert into # AvgTempStream from every ( e1 =# AvgTempStream ), e2 =# AvgTempStream [ e1 . avgTemp + 5 avgTemp ] select e1 . deviceID , e1 . avgTemp as initialAvgTemp , e2 . avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end ;","title":"Purge Partition"},{"location":"documentation/query-guide-5.x/#table","text":"A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table table name ( attribute name attribute type , attribute name attribute type , ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int , type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @ PrimaryKey ( symbol ) define table StockTable ( symbol string , price float , volume long ); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @ Index ( roomNo ) define table RoomTypeTable ( roomNo int , type string );","title":"Table"},{"location":"documentation/query-guide-5.x/#store","text":"Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @ store ( type = store_type , static . option . key1 = static_option_value1 , static . option . keyN = static_option_valueN ) define table TableName ( attribute1 Type1 , attributeN TypeN ); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @ Store ( type = rdbms , jdbc . url = jdbc:mysql://localhost:3306/hotel , username = siddhi , password = 123 , jdbc . driver . name = com.mysql.jdbc.Driver ) define table RoomTypeTable ( roomNo int , type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) Solr MongoDB HBase Redis Cassandra Operators on Table (and Store) The following operators can be performed on tables (and stores).","title":"Store"},{"location":"documentation/query-guide-5.x/#insert","text":"This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from input stream select attribute name , attribute name , ... insert into table Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific output event types. For more information, see output event type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable ;","title":"Insert"},{"location":"documentation/query-guide-5.x/#join-table","text":"This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from input stream join table on condition select ( input stream | table ). attribute name , ( input stream | table ). attribute name , ... insert into output stream Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable ( roomNo int , type string ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream join RoomTypeTable on RoomTypeTable . roomNo == TempStream . roomNo select deviceID , RoomTypeTable . type as roomType , type , temp having roomType == server-room insert into ServerRoomTempStream ; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table.","title":"Join (Table)"},{"location":"documentation/query-guide-5.x/#delete","text":"To delete selected events that are stored in a table. Syntax from input stream select attribute name , attribute name , ... delete table ( for output event type ) ? on condition The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific output event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type Note Table attributes must be always referred to with the table name as follows: table name . attibute name Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable ( roomNo int , type string ); define stream DeleteStream ( roomNumber int ); from DeleteStream delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ;","title":"Delete"},{"location":"documentation/query-guide-5.x/#update","text":"This operator updates selected event attributes stored in a table based on a condition. Syntax from input stream select attribute name , attribute name , ... update table ( for output event type ) ? set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see output event type . Note Table attributes must be always referred to with the table name as shown below: table name . attibute name . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable ( roomNo int , people int ); define stream UpdateStream ( roomNumber int , arrival int , exit int ); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable . people = RoomOccupancyTable . people + arrival - exit on RoomOccupancyTable . roomNo == roomNumber ;","title":"Update"},{"location":"documentation/query-guide-5.x/#update-or-insert","text":"This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from input stream select attribute name , attribute name , ... update or insert into table ( for output event type ) ? set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific output event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see output event type . Note Table attributes should be always referred to with the table name as table name . attibute name . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable ( roomNo int , type string , assignee string ); define stream RoomAssigneeStream ( roomNumber int , type string , assignee string ); from RoomAssigneeStream select roomNumber as roomNo , type , assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ;","title":"Update or Insert"},{"location":"documentation/query-guide-5.x/#in","text":"This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from input stream [ condition in table ] select attribute name , attribute name , ... insert into output stream The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: table . attibute name . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable ( roomNo int ); define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream [ ServerRoomTable . roomNo == roomNo in ServerRoomTable ] insert into ServerRoomTempStream ;","title":"In"},{"location":"documentation/query-guide-5.x/#named-aggregation","text":"Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @ store ( type = store type , ...) @ purge ( enable = true or false , interval = purging interval , @ retentionPeriod ( granularity = retention period , ...) ) define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: ' @purge (enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. aggregator name This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. input stream The stream that feeds the aggregation. Note! this stream should be already defined. group by attribute name The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by timestamp attribute This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are yyyy - MM - dd HH : mm : ss (if time is in GMT) and yyyy - MM - dd HH : mm : ss Z (if time is not in GMT), here the ISO 8601 UTC offset must be provided for Z . (e.g., +05:30 , -11:00 ). time periods Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); @ purge ( enable = true , interval = 10 sec , @ retentionPeriod ( sec = 120 sec , min = 24 hours , hours = 30 days , days = 1 year , months = all , years = all )) define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ;","title":"Named Aggregation"},{"location":"documentation/query-guide-5.x/#distributed-aggregation","text":"Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database( @store ). Syntax @ store ( type = store type , ...) @ PartitionById define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency.","title":"Distributed Aggregation"},{"location":"documentation/query-guide-5.x/#join-aggregation","text":"This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from input stream join aggrigation on join condition within time range per time granularity select attribute name , attribute name , ... insert into output stream ; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within time range This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per time granularity This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP , symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream ( symbol string , value int ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 2014-02-15 **:**:** +05:30 per hours select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream ( symbol string , value int , perValue string ); from StockStream as S join TradeAggregation as T on S . symbol == T . symbol within 1496200000000 L , 1596434876000 L per S . perValue select S . symbol , T . total , T . avgPrice insert into AggregateStockStream ; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation.","title":"Join (Aggregation)"},{"location":"documentation/query-guide-5.x/#named-window","text":"A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window window name ( attribute name attribute type , attribute name attribute type , ... ) window type ( parameter , parameter , \u2026 ) output event type ; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . window type ( parameter , ...) The window type associated with the window and its parameters. output output event type This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see output event type . Examples Returning all output when events arrive and when events expire from the window. In this query, the output event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ); + Returning an output only when events expire from the window. In this query, the output event type of the window is `expired events`. Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow ( name string , value float , roomNo int , deviceID string ) timeBatch ( 1 second ) output expired events ; Operators on Named Windows The following operators can be performed on named windows.","title":"Named Window"},{"location":"documentation/query-guide-5.x/#insert_1","text":"This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from input stream select attribute name , attribute name , ... insert into window To insert only events of a specific output event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see output event type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream ( tempId string , temp double ); define window OneMinTempWindow ( tempId string , temp double ) time ( 1 min ); from TempStream select * insert into OneMinTempWindow ;","title":"Insert"},{"location":"documentation/query-guide-5.x/#join-window","text":"To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from input stream join window on condition select ( input stream | window ). attribute name , ( input stream | window ). attribute name , ... insert into output stream Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow ( roomNo int , temp double ) time ( 2 min ); define stream CheckStream ( requestId string ); from CheckStream as C join TwoMinTempWindow as T on T . temp 40 select requestId , count ( T . temp ) as count insert into HighTempCountStream ; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window.","title":"Join (Window)"},{"location":"documentation/query-guide-5.x/#from","text":"A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from window select attribute name , attribute name , ... insert into output stream Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow ( roomNo int , temp double ) time ( 5 min ); from FiveMinTempWindow select max ( temp ) as maxValue , roomNo insert into MaxSensorReadingStream ;","title":"From"},{"location":"documentation/query-guide-5.x/#trigger","text":"Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given time interval , or for a given ' cron expression ' . Syntax The syntax for a trigger definition is as follows. define trigger trigger name at ( start | every time interval | cron expression ); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream trigger name ( triggered_time long ); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every time interval An event is triggered periodically at the given time interval. ' cron expression ' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min ; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at 0 15 10 ? * MON-FRI ;","title":"Trigger"},{"location":"documentation/query-guide-5.x/#script","text":"Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function function name [ language name ] return return type { operation of the function } ; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn [ javascript ] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3 ; return responce ; } ; define stream TempStream ( deviceID long , roomNo int , temp double ); from TempStream select concatFn ( roomNo , - , deviceID ) as id , temp insert into DeviceTempStream ;","title":"Script"},{"location":"documentation/query-guide-5.x/#store-query","text":"Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime . query ( store query );","title":"Store Query"},{"location":"documentation/query-guide-5.x/#tablewindow-select","text":"The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from table / window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo = 10 ; select roomNo , type","title":"(Table/Window) Select"},{"location":"documentation/query-guide-5.x/#aggregation-select","text":"The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... group by ? having ? order by ? limit ? Example Following aggregation definition will be used for the examples. define stream TradeStream ( symbol string , price double , volume long , timestamp long ); define aggregation TradeAggregation from TradeStream select symbol , avg ( price ) as avgPrice , sum ( price ) as total group by symbol aggregate by timestamp every sec ... year ; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select symbol , total , avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == FB within 2014-02-15 **:**:** +05:30 per hours select symbol , total , avgPrice ;","title":"(Aggregation) Select"},{"location":"documentation/query-guide-5.x/#insert_2","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo , 2 as people insert into RoomOccupancyTable","title":"Insert"},{"location":"documentation/query-guide-5.x/#delete_1","text":"The DELETE store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable . roomNo == roomNumber ; delete RoomTypeTable on RoomTypeTable . roomNo == 10 ;","title":"Delete"},{"location":"documentation/query-guide-5.x/#update_1","text":"The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select attribute name , attribute name , ... ? update table set table . attribute name = ( attribute name | expression ) ? , table . attribute name = ( attribute name | expression ) ? , ... on condition The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber , 1 as arrival update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + arrival on RoomTypeTable . roomNo == roomNumber ; update RoomTypeTable set RoomTypeTable . people = RoomTypeTable . people + 1 on RoomTypeTable . roomNo == 10 ;","title":"Update"},{"location":"documentation/query-guide-5.x/#update-or-insert_1","text":"This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: table name . attibute name . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo , single as type , abc as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable . assignee = assignee on RoomAssigneeTable . roomNo == roomNo ;","title":"Update or Insert"},{"location":"documentation/query-guide-5.x/#extensions","text":"Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; namespace : function name ( parameter , parameter , ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults( parameter , parameter , ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream [ price = 20 ] # window . foo : unique ( symbol ) select symbol , price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further.","title":"Extensions"},{"location":"documentation/query-guide-5.x/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N {_nameSpaceOfFunction} artifactId Artifact Id of the project N siddhi-execution-{_nameSpaceOfFunction} classNameOfAggregateFunction Class name of the Aggregate Function N ${_nameOfFunction}AggregateFunction classNameOfFunction Class name of the Function N ${_nameOfFunction}Function classNameOfStreamFunction Class name of the Stream Function N ${_nameOfFunction}StreamFunction classNameOfStreamProcessor Class name of the Stream Processor N ${_nameOfFunction}StreamProcessor classNameOfWindow Class name of the Window N ${_nameOfFunction}Window To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N {_IOType} artifactId Artifact Id of the project N siddhi-io-{_IOType} classNameOfSink Class name of the Sink N {_IOType}Sink classNameOfSource Class name of the Source N {_IOType}Source To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N {_mapType} artifactId Artifact Id of the project N siddhi-map-{_mapType} classNameOfSinkMapper Class name of the Sink Mapper N {_mapType}SinkMapper classNameOfSourceMapper Class name of the Source Mapper N {_mapType}SourceMapper To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N {_nameOfScript} artifactId Artifact Id of the project N siddhi-script-{_nameOfScript} classNameOfScript Class name of the Script N Eval{_nameOfScript} To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N {_storeType} artifactId Artifact Id of the project N siddhi-store-{_storeType} className Class name of the Store N {_storeType}EventTable To confirm that all property values are correct, type Y in the console. If not, press N .","title":"Writing Custom Extensions"},{"location":"documentation/query-guide-5.x/#configuring-and-monitoring-siddhi-applications","text":"","title":"Configuring and Monitoring Siddhi Applications"},{"location":"documentation/query-guide-5.x/#multi-threading-and-asynchronous-processing","text":"When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @ Async ( buffer . size = 256 , workers = 2 , batch . size . max = 5 ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size","title":"Multi-threading and Asynchronous Processing"},{"location":"documentation/query-guide-5.x/#statistics","text":"Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @ app : statistics ( reporter = console ) The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps. SiddhiAppName .Siddhi. Component Type . Component Name . Metrics name The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @ App : name ( TestMetrics ) @ App : Statistics ( reporter = console ) define stream TestStream ( message string ); @ info ( name = logQuery ) from TestSream # log ( Message: ) insert into TempSream ; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"documentation/query-guide-5.x/#event-playback","text":"When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Event Playback"},{"location":"documentation/api/latest/","text":"API Docs - v5.0.0 Core and (Aggregate Function) Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events. batch (Window) A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink inMemory (Sink) In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper passThrough (Sink Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source inMemory (Source) In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper passThrough (Source Mapper) Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type= passThrough ) Examples EXAMPLE 1 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"API Docs - v5.0.0"},{"location":"documentation/api/latest/#api-docs-v500","text":"","title":"API Docs - v5.0.0"},{"location":"documentation/api/latest/#core","text":"","title":"Core"},{"location":"documentation/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax BOOL and( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"documentation/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax DOUBLE avg( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"documentation/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Syntax LONG count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"documentation/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax LONG distinctCount( INT|LONG|DOUBLE|FLOAT|STRING arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"documentation/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT max( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"documentation/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT maxForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"documentation/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax INT|LONG|DOUBLE|FLOAT min( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"documentation/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax INT|LONG|DOUBLE|FLOAT minForever( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"documentation/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax BOOL or( BOOL arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"documentation/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax DOUBLE stdDev( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"documentation/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax LONG|DOUBLE sum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"documentation/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax OBJECT unionSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"documentation/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax STRING UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, string ) as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"documentation/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT cast( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT to.be.caster, STRING cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, double ) as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"documentation/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT coalesce( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce( 123 , null, 789 ) as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"documentation/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL convert( INT|LONG|DOUBLE|FLOAT|STRING|BOOL to.be.converted, STRING converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, double ) as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, int ) as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"documentation/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax OBJECT createSet( INT|LONG|DOUBLE|FLOAT|STRING|BOOL input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"documentation/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax LONG currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"documentation/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT attribute, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"documentation/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax LONG eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"documentation/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT ifThenElse( BOOL condition, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT if.expression, INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(sensorValue 35, High , Low ) as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = query1 ) from sensorEventStream select sensorValue, ifThenElse(voltage 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = query1 ) from userEventStream select userName, ifThenElse(password == admin , true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"documentation/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax BOOL instanceOfBoolean( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"documentation/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax BOOL instanceOfDouble( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"documentation/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax BOOL instanceOfFloat( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"documentation/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax BOOL instanceOfInteger( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"documentation/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax BOOL instanceOfLong( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"documentation/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax BOOL instanceOfString( INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"documentation/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT maximum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"documentation/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax INT|LONG|DOUBLE|FLOAT minimum( INT|LONG|DOUBLE|FLOAT arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = query1 ) from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"documentation/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax INT sizeOfSet( OBJECT set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"documentation/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart( DOUBLE theta, DOUBLE rho, DOUBLE z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"documentation/api/latest/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log( STRING priority, STRING log.message, BOOL is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log( INFO , Sample Event : , true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log( Sample Event : , true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log( Sample Event : , fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log( Sample Event : ) select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"documentation/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"documentation/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron( STRING cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#cron( */5 * * * * ? ) select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron( */5 * * * * ? ); @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"documentation/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay( INT|LONG|TIME window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name= query1 ) from PurchaseStream select symbol, volume insert into delayWindow; @info(name= query2 ) from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"documentation/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime( LONG timestamp, INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"documentation/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch( LONG timestamp, INT|LONG|TIME window.time, INT|LONG|TIME start.time, INT|LONG|TIME timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"documentation/api/latest/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent( INT event.count, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = query1 ) from purchase[price = 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = query1 ) from purchase[price = 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"documentation/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length( INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = query0 ) from StockEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"documentation/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch( INT window.length, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"documentation/api/latest/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent( DOUBLE support.threshold, DOUBLE error.bound, STRING attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = query0 ) from purchase[price = 30] insert into purchaseWindow; @info(name = query1 ) from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"documentation/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session( INT|LONG|TIME window.session, STRING window.key, INT|LONG|TIME window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name= query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"documentation/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort( INT window.length, STRING attribute, STRING order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, asc ); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"documentation/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time( INT|LONG|TIME window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"documentation/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch( INT|LONG|TIME window.time, INT start.time, BOOL stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = query1 ) from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = query0 ) from InputEventStream insert into StockEventWindow; @info(name = query1 ) from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"documentation/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength( INT|LONG|TIME window.time, INT window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = query0 ) from cseEventStream insert into cseEventWindow; @info(name = query1 ) from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"documentation/api/latest/#sink","text":"","title":"Sink"},{"location":"documentation/api/latest/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"documentation/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type= log , priority= STRING , prefix= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type= log , prefix= My Log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type= log , priority= DEBUG ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form Siddhi App Name : Stream Name EXAMPLE 3 @sink(type= log , prefix= My Log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type= log ) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"documentation/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"documentation/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type= passThrough ) Examples EXAMPLE 1 @sink(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"documentation/api/latest/#source","text":"","title":"Source"},{"location":"documentation/api/latest/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type= inMemory , topic= STRING , @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type= inMemory , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"documentation/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"documentation/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type= passThrough ) Examples EXAMPLE 1 @source(type= tcp , @map(type= passThrough )) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/","text":"Siddhi 5.x as a Docker Microservice This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/#siddhi-5x-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v local-siddhi-file-path : siddhi-file-mount-path -v local-conf-file-path : conf-file-mount-path siddhiio/siddhi-runner-alpine:latest -Dapps= siddhi-file-mount-path -Dconfig= conf-file-mount-path E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.x as a Docker Microservice"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/#samples","text":"","title":"Samples"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v local-absolute-siddhi-file-path /ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v local-absolute-config-yaml-path /TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"documentation/user-guide/siddhi-as-a-docker-microservice-5.x/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL= to email address EMAIL_ADDRESS= gmail address EMAIL_USERNAME= gmail username EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v local-absolute-siddhi-file-path /TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v local-absolute-config-yaml-path /EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL= to email address -e EMAIL_ADDRESS= gmail address -e EMAIL_USERNAME= gmail username -e EMAIL_PASSWORD= gmail password siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"documentation/user-guide/siddhi-as-a-java-library-5.x/","text":"Siddhi 5.x as a Java library Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency Sample Sample Java class using Siddhi is as follows.","title":"Siddhi Java library"},{"location":"documentation/user-guide/siddhi-as-a-java-library-5.x/#siddhi-5x-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). dependency groupId io.siddhi /groupId artifactId siddhi-core /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-api /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-query-compiler /artifactId version 5.x.x /version /dependency dependency groupId io.siddhi /groupId artifactId siddhi-annotations /artifactId version 5.x.x /version /dependency","title":"Siddhi 5.x as a Java library"},{"location":"documentation/user-guide/siddhi-as-a-java-library-5.x/#sample","text":"Sample Java class using Siddhi is as follows.","title":"Sample"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/","text":"Siddhi 5.x as a Kubernetes Microservice This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \" \" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user= Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Install Siddhi Operator To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m Deploy and run Siddhi App Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 10d monitor-app ClusterIP 10.101.242.132 none 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 none 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 pending 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Get Siddhi process status List Siddhi processes List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m View Siddhi process configs Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: none Annotations: none API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name( MonitorApp ) @App:description( Description of the plan ) @sink(type= log , prefix= LOGGER ) @source(type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= ${BASIC_AUTH_ENABLED} , @map(type= json )) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name= monitored-filter ) from DevicePowerStream[type == monitored ] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: nil Status: Running Events: none Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: 497702 selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: false pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: @App:name(\\ MonitorApp\\ )\\n@App:description(\\ Description of the plan\\ ) \\n\\n@sink(type= log , prefix= LOGGER )\\n@source(type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= ${BASIC_AUTH_ENABLED} , @map(type= json ))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name= monitored-filter )\\nfrom DevicePowerStream[type == monitored ]\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running View Siddhi process logs To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain org.wso2.carbon.metrics [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy and run Siddhi App using configmaps Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file= absolute-file-path /MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy Siddhi Apps without Ingress creation By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below. Deploy and run Siddhi App with HTTPS Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj /CN=siddhi/O=siddhi After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain org.wso2.carbon.metrics [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Siddhi Kubernetes Microservice"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#siddhi-5x-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.x as a Kubernetes Microservice"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \" \" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user= Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation .","title":"Prerequisites"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m","title":"Install Siddhi Operator"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f absolute-yaml-file-path /monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 none 443/TCP 10d monitor-app ClusterIP 10.101.242.132 none 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 none 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 pending 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m","title":"List Siddhi processes"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#view-siddhi-process-configs","text":"Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: none Annotations: none API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name( MonitorApp ) @App:description( Description of the plan ) @sink(type= log , prefix= LOGGER ) @source(type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= ${BASIC_AUTH_ENABLED} , @map(type= json )) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name= monitored-filter ) from DevicePowerStream[type == monitored ] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: nil Status: Running Events: none Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: 497702 selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: false pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: @App:name(\\ MonitorApp\\ )\\n@App:description(\\ Description of the plan\\ ) \\n\\n@sink(type= log , prefix= LOGGER )\\n@source(type= http , receiver.url= ${RECEIVER_URL} , basic.auth.enabled= ${BASIC_AUTH_ENABLED} , @map(type= json ))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name= monitored-filter )\\nfrom DevicePowerStream[type == monitored ]\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running","title":"View Siddhi process configs"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs pod name command. Here pod name should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain org.wso2.carbon.metrics [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"View Siddhi process logs"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#deploy-and-run-siddhi-app-using-configmaps","text":"Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file= absolute-file-path /MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f absolute-yaml-file-path /monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file= DIRECTORY_PATH command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs pod name command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App using configmaps"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below.","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"documentation/user-guide/siddhi-as-a-kubernetes-microservice-5.x/#deploy-and-run-siddhi-app-with-https","text":"Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj /CN=siddhi/O=siddhi After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H Content-Type: application/json \\ -d { type : monitored , deviceID : 001 , power : 341 } View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain org.wso2.carbon.metrics [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/","text":"Siddhi 5.x as a Local Microservice This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples Running Siddhi App Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/#siddhi-5x-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= siddhi-file -Dconfig= config-yaml-file Windows : bin\\runner.bat -Dapps= siddhi-file -Dconfig= config-yaml-file Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps= siddhi-apps-directory Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.x as a Local Microservice"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/#samples","text":"","title":"Samples"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /CountOverTime.siddhi Windows : bin\\runner.bat -Dapps= absolute-siddhi-file-path \\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-siddhi-file-path /ConsumeAndStore.siddhi \\ -Dconfig= absolute-config-yaml-path /TestDb.yaml Windows : bin\\runner.sh -Dapps= absolute-siddhi-file-path \\ConsumeAndStore.siddhi ^ -Dconfig= absolute-config-yaml-path \\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"documentation/user-guide/siddhi-as-a-local-microservice-5.x/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL= to email address export EMAIL_ADDRESS= gmail address export EMAIL_USERNAME= gmail username export EMAIL_PASSWORD= gmail password Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL= to email address -DEMAIL_ADDRESS= gmail address -DEMAIL_USERNAME= gmail username -DEMAIL_PASSWORD= gmail password to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps= absolute-file-path /TemplatedFilterAndEmail.siddhi \\ -Dconfig= absolute-config-yaml-path /EmailConfig.yaml Windows : bin\\runner.bat -Dapps= absolute-file-path \\TemplatedFilterAndEmail.siddhi ^ -Dconfig= absolute-config-yaml-path \\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"documentation/user-guide/user-guide-introduction-5.x/","text":"Siddhi 5.x User Guide This section provides information on developing and running Siddhi. Siddhi Application A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide Execution Environments Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) System Requirements For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"documentation/user-guide/user-guide-introduction-5.x/#siddhi-5x-user-guide","text":"This section provides information on developing and running Siddhi.","title":"Siddhi 5.x User Guide"},{"location":"documentation/user-guide/user-guide-introduction-5.x/#siddhi-application","text":"A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide","title":"Siddhi Application"},{"location":"documentation/user-guide/user-guide-introduction-5.x/#execution-environments","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Execution Environments"},{"location":"documentation/user-guide/user-guide-introduction-5.x/#system-requirements","text":"For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"}]}